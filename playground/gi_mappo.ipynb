{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph isomorphism PPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_CPU = True\n",
    "\n",
    "SEED = 349287\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/sam/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from tensordict.nn import (\n",
    "    TensorDictModule,\n",
    "    TensorDictModuleBase,\n",
    "    TensorDictSequential,\n",
    "    ProbabilisticTensorDictSequential,\n",
    ")\n",
    "from tensordict.nn.distributions import CompositeDistribution\n",
    "from tensordict.tensordict import TensorDict, TensorDictBase\n",
    "from tensordict.nn import InteractionType\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.data.tensor_specs import (\n",
    "    CompositeSpec,\n",
    "    DiscreteTensorSpec,\n",
    "    BinaryDiscreteTensorSpec,\n",
    "    MultiDiscreteTensorSpec,\n",
    "    TensorSpec,\n",
    "    UnboundedContinuousTensorSpec,\n",
    "    Box,\n",
    ")\n",
    "from torchrl.envs import EnvBase\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.modules import ProbabilisticActor, ActorValueOperator\n",
    "from torchrl.objectives import ValueEstimators\n",
    "\n",
    "from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
    "from torch_geometric.data import Batch as GeometricBatch, Data as GeometricData\n",
    "\n",
    "from jaxtyping import Float, Int, Bool\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pvg.graph_isomorphism import (\n",
    "    GraphIsomorphismAgentBody,\n",
    "    GraphIsomorphismAgentPolicyHead,\n",
    "    GraphIsomorphismAgentValueHead,\n",
    ")\n",
    "from pvg.parameters import Parameters\n",
    "from pvg.graph_isomorphism.data import GraphIsomorphismData, GraphIsomorphismDataset\n",
    "from pvg.utils.data import gi_data_to_tensordict\n",
    "from pvg.utils.torchrl_objectives import ClipPPOLossMultipleActions\n",
    "from pvg.constants import VERIFIER_AGENT_NUM, PROVER_AGENT_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch_generator = torch.Generator().manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if not FORCE_CPU and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Parameters(\n",
    "    scenario=\"graph_isomorphism\",\n",
    "    trainer=\"ppo\",\n",
    "    dataset=\"eru10000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyMatrixBox(Box):\n",
    "    \"\"\"An abstract representation of the space of adjacency matrices.\"\"\"\n",
    "\n",
    "    def __init__(self, max_num_nodes: int):\n",
    "        self.max_num_nodes = max_num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyMatrixSpec(TensorSpec):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_num_nodes: int,\n",
    "        shape: torch.Size | None = None,\n",
    "        device: Optional[torch.device | str | int] = None,\n",
    "        dtype: str | torch.dtype = torch.int32,\n",
    "    ):\n",
    "        self.max_num_nodes = max_num_nodes\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        if shape is None:\n",
    "            self.shape = torch.Size([max_num_nodes, max_num_nodes])\n",
    "        else:\n",
    "            if shape[-2:] != (max_num_nodes, max_num_nodes):\n",
    "                raise ValueError(\n",
    "                    f\"The last two dimensions of the shape must be {max_num_nodes}. \"\n",
    "                    f\"Got {shape[-2:]}.\"\n",
    "                )\n",
    "            self.shape = torch.Size(shape)\n",
    "\n",
    "        self.space = AdjacencyMatrixBox(max_num_nodes)\n",
    "\n",
    "    def is_in(self, val: torch.Tensor) -> bool:\n",
    "        \"\"\"Check if a value is a valid adjacency matrix.\"\"\"\n",
    "\n",
    "        # Basic type checks\n",
    "        if not isinstance(val, torch.Tensor):\n",
    "            return False\n",
    "        if val.shape[-2:] != (self.max_num_nodes, self.max_num_nodes):\n",
    "            return False\n",
    "        if val.dtype != self.dtype:\n",
    "            return False\n",
    "\n",
    "        # Make sure the values are either 0 or 1\n",
    "        if not torch.all(torch.isin(val, torch.tensor([0, 1], device=self.device))):\n",
    "            return False\n",
    "\n",
    "        # Make sure the matrix is symmetric\n",
    "        if not torch.all(val.transpose(-1, -2) == val):\n",
    "            return False\n",
    "\n",
    "        # Make sure the diagonal is all zeros\n",
    "        if not torch.all(torch.isin(torch.diagonal(val, dim1=-2, dim2=-1), 0)):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def rand(self, shape: Optional[list[int] | torch.Size] = None) -> torch.Tensor:\n",
    "        \"\"\"Generate a random 1/2 Erdos-Renyi adjacency matrix.\"\"\"\n",
    "\n",
    "        if shape is None:\n",
    "            shape = shape = torch.Size([])\n",
    "\n",
    "        adjacency_values = torch.rand(*shape, *self.shape, device=device)\n",
    "        adjacency = (adjacency_values < 0.5).to(self.dtype)\n",
    "        adjacency = adjacency.triu(diagonal=1)\n",
    "        adjacency += adjacency.transpose(1, 2).clone()\n",
    "\n",
    "        return adjacency\n",
    "\n",
    "    def _project(self, val: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Project a value to the space of valid adjacency matrices.\"\"\"\n",
    "\n",
    "        # Symmetrize the matrix\n",
    "        val = (val + val.transpose(1, 2)) / 2\n",
    "\n",
    "        # Make sure the diagonal is all zeros\n",
    "        val[..., torch.arange(self.max_num_nodes), torch.arange(self.max_num_nodes)] = 0\n",
    "\n",
    "        # Make sure the values are either 0 or 1\n",
    "        return torch.clamp(torch.round(val), min=0, max=1).to(self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable batch-size data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forgetful_cycle(iterable):\n",
    "    \"\"\"A version of cycle that doesn't save copies of the values\"\"\"\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableGeometricDataCycler:\n",
    "    \"\"\"A loader that cycles through geometric data, but allows the batch size to vary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataloader : GeometricDataLoader\n",
    "        The base dataloader to use. This dataloader will be cycled through.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataloader: GeometricDataLoader):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataloader_iter = iter(forgetful_cycle(self.dataloader))\n",
    "        self.remainder: Optional[list] = None\n",
    "\n",
    "    def get_batch(self, batch_size: int) -> GeometricBatch:\n",
    "        \"\"\"Get a batch of data from the dataloader with the given batch size.\n",
    "\n",
    "        If the dataloader is exhausted, it will be reset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The size of the batch to return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        batch : Tensor\n",
    "            A batch of data with the given batch size.\n",
    "        \"\"\"\n",
    "\n",
    "        left_to_sample = batch_size\n",
    "        batch_components = []\n",
    "\n",
    "        # Start by sampling from the remainder from the previous sampling\n",
    "        if self.remainder is not None:\n",
    "            batch_components.extend(self.remainder[:left_to_sample])\n",
    "            if len(self.remainder) <= left_to_sample:\n",
    "                left_to_sample -= len(self.remainder)\n",
    "                self.remainder = None\n",
    "            else:\n",
    "                self.remainder = self.remainder[left_to_sample:]\n",
    "                left_to_sample = 0\n",
    "\n",
    "        # Keep sampling batches until we have enough\n",
    "        while left_to_sample > 0:\n",
    "            batch = next(self.dataloader_iter)\n",
    "            batch_components.extend(batch[:left_to_sample])\n",
    "            if len(batch) <= left_to_sample:\n",
    "                left_to_sample -= len(batch)\n",
    "            else:\n",
    "                self.remainder = batch[left_to_sample:]\n",
    "                left_to_sample = 0\n",
    "\n",
    "        # Concatenate the batch components into a single batch\n",
    "        batch = GeometricBatch.from_data_list(\n",
    "            batch_components, follow_batch=self.dataloader.follow_batch\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.dataloader!r})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphIsomorphismEnv(EnvBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Parameters,\n",
    "        device: torch.device | str = device,\n",
    "        int_dtype: torch.dtype = torch.int,\n",
    "    ):\n",
    "        super().__init__(device=device)\n",
    "        self.params = params\n",
    "        self.int_dtype = int_dtype\n",
    "\n",
    "        # Create a random number generator\n",
    "        self.rng = torch.Generator(device=device)\n",
    "\n",
    "        # Load the dataset\n",
    "        self.dataset = GraphIsomorphismDataset(params)\n",
    "        self.data_cycler: Optional[VariableGeometricDataCycler] = None\n",
    "\n",
    "        # Compute the maximum number of nodes in the dataset\n",
    "        self.max_num_nodes = 0\n",
    "        for data in self.dataset:\n",
    "            self.max_num_nodes = max(\n",
    "                self.max_num_nodes, data.x_a.shape[0], data.x_b.shape[0]\n",
    "            )\n",
    "\n",
    "        # The number of environments is the number of episodes we can fit in a batch\n",
    "        self.num_envs = params.ppo.frames_per_batch // params.max_message_rounds\n",
    "        self.batch_size = (self.num_envs,)\n",
    "\n",
    "        # The spec for the observation space: agents see the adjacency matrix and the\n",
    "        # messages sent so far. The \"message\" field contains the most recent message.\n",
    "        self.observation_spec = CompositeSpec(\n",
    "            adjacency=AdjacencyMatrixSpec(\n",
    "                self.max_num_nodes,\n",
    "                shape=(self.num_envs, 2, self.max_num_nodes, self.max_num_nodes),\n",
    "                dtype=self.int_dtype,\n",
    "            ),\n",
    "            x=BinaryDiscreteTensorSpec(\n",
    "                params.max_message_rounds,\n",
    "                shape=(\n",
    "                    self.num_envs,\n",
    "                    2,\n",
    "                    self.max_num_nodes,\n",
    "                    params.max_message_rounds,\n",
    "                ),\n",
    "                dtype=torch.float,\n",
    "            ),\n",
    "            node_mask=BinaryDiscreteTensorSpec(\n",
    "                self.max_num_nodes,\n",
    "                shape=(\n",
    "                    self.num_envs,\n",
    "                    2,\n",
    "                    self.max_num_nodes,\n",
    "                ),\n",
    "                dtype=torch.bool,\n",
    "            ),\n",
    "            message=DiscreteTensorSpec(\n",
    "                2 * self.max_num_nodes,\n",
    "                shape=(self.num_envs,),\n",
    "                dtype=torch.long,\n",
    "            ),\n",
    "            round=DiscreteTensorSpec(\n",
    "                params.max_message_rounds,\n",
    "                shape=(self.num_envs,),\n",
    "                dtype=self.int_dtype,\n",
    "            ),\n",
    "            shape=(self.num_envs,),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # The spec for the state space: the true label and the round number\n",
    "        self.state_spec = CompositeSpec(\n",
    "            y=BinaryDiscreteTensorSpec(\n",
    "                1,\n",
    "                shape=(self.num_envs, 1),\n",
    "                dtype=self.int_dtype,\n",
    "            ),\n",
    "            shape=(self.num_envs,),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Each action space has shape (batch_size, num_agents). Each agent chooses both\n",
    "        # a node and a decision: reject, accept or continue (represented as 0, 1 or 2).\n",
    "        # The node is is a number between 0 and 2 * max_num_nodes - 1. If it is less\n",
    "        # than max_num_nodes, it is a node in the first graph, otherwise it is a node in\n",
    "        # the second graph. The verifier is agent 0 and the prover is agent 1.\n",
    "        self.action_spec = CompositeSpec(\n",
    "            agents=CompositeSpec(\n",
    "                node_selected=DiscreteTensorSpec(\n",
    "                    2 * self.max_num_nodes,\n",
    "                    shape=(self.num_envs, 2),\n",
    "                    dtype=self.int_dtype,\n",
    "                ),\n",
    "                decision=DiscreteTensorSpec(\n",
    "                    3,\n",
    "                    shape=(self.num_envs, 2),\n",
    "                    dtype=self.int_dtype,\n",
    "                ),\n",
    "                shape=(self.num_envs,),\n",
    "            ),\n",
    "            shape=(self.num_envs,),\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.reward_spec = CompositeSpec(\n",
    "            agents=CompositeSpec(\n",
    "                reward=UnboundedContinuousTensorSpec(shape=(self.num_envs, 2)),\n",
    "                shape=(self.num_envs,),\n",
    "            ),\n",
    "            shape=(self.num_envs,),\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.done_spec = CompositeSpec(\n",
    "            done=BinaryDiscreteTensorSpec(\n",
    "                self.num_envs, shape=(self.num_envs,), dtype=torch.bool\n",
    "            ),\n",
    "            shape=(self.num_envs,),\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _step(self, tensordict: TensorDictBase) -> TensorDictBase:\n",
    "        \"\"\"Perform a step in the environment.\"\"\"\n",
    "\n",
    "        # Extract the tensors from the dict\n",
    "        y: Int[Tensor, \"batch\"] = tensordict[\"y\"]\n",
    "        x: Float[Tensor, \"batch graph node message_round\"] = tensordict[\"x\"]\n",
    "        round: Int[Tensor, \"batch\"] = tensordict[\"round\"]\n",
    "        node_selected: Int[Tensor, \"batch agent\"] = tensordict[\n",
    "            \"agents\", \"node_selected\"\n",
    "        ]\n",
    "        decision: Int[Tensor, \"batch agent\"] = tensordict[\"agents\", \"decision\"]\n",
    "        done: Bool[Tensor, \"batch\"] = tensordict[\"done\"]\n",
    "\n",
    "        # Compute index of the agent whose turn it is. The prover goes first.\n",
    "        agent_index: Int[Tensor, \"batch\"] = (round % 2)\n",
    "        if PROVER_AGENT_NUM == 1:\n",
    "            agent_index = 1 - agent_index\n",
    "\n",
    "        # Determine which graph contains the selected node and which node it is there\n",
    "        # (batch agent)\n",
    "        which_graph = node_selected >= self.max_num_nodes\n",
    "        # (batch agent)\n",
    "        graph_node = torch.where(\n",
    "            which_graph, node_selected - self.max_num_nodes, node_selected\n",
    "        )\n",
    "\n",
    "        # Write the node selected by the agent whose turn it is as a (one-hot) message\n",
    "        x[\n",
    "            torch.arange(x.shape[0]),\n",
    "            which_graph[torch.arange(which_graph.shape[0]), agent_index].int(),\n",
    "            graph_node[torch.arange(which_graph.shape[0]), agent_index],\n",
    "            round,\n",
    "        ] = 1\n",
    "\n",
    "        # Set the node selected by the agent whose turn it is as the message\n",
    "        message = node_selected[\n",
    "            torch.arange(node_selected.shape[0]), agent_index\n",
    "        ].long()\n",
    "\n",
    "        # If the verifier has made a guess, compute the reward and terminate the episode\n",
    "        verifier_decision_made = (agent_index == VERIFIER_AGENT_NUM) & (\n",
    "            decision[:, VERIFIER_AGENT_NUM] != 2\n",
    "        )\n",
    "        done = done | verifier_decision_made\n",
    "        reward_verifier = (\n",
    "            verifier_decision_made & (decision[:, VERIFIER_AGENT_NUM] == y.squeeze())\n",
    "        ).float()\n",
    "        reward_verifier = reward_verifier * self.params.verifier_reward\n",
    "        reward_prover = (\n",
    "            verifier_decision_made & (decision[:, VERIFIER_AGENT_NUM] == 1)\n",
    "        ).float()\n",
    "        reward_prover = reward_prover * self.params.prover_reward\n",
    "\n",
    "        # If we reach the end of the episode and the verifier has not made a guess,\n",
    "        # terminate it with a negative reward for the verifier\n",
    "        done = done | (round >= self.params.max_message_rounds - 1)\n",
    "        reward_verifier[\n",
    "            (round >= self.params.max_message_rounds - 1) & ~verifier_decision_made\n",
    "        ] = self.params.verifier_terminated_penalty\n",
    "\n",
    "        # Stack the rewards for the two agents\n",
    "        if PROVER_AGENT_NUM == 1:\n",
    "            reward = torch.stack([reward_verifier, reward_prover], dim=-1)\n",
    "        else:\n",
    "            reward = torch.stack([reward_prover, reward_verifier], dim=-1)\n",
    "\n",
    "        # Put everything together\n",
    "        next = TensorDict(\n",
    "            dict(\n",
    "                adjacency=tensordict[\"adjacency\"],\n",
    "                x=x,\n",
    "                node_mask=tensordict[\"node_mask\"],\n",
    "                message=message,\n",
    "                round=round + 1,\n",
    "                done=done,\n",
    "                agents=TensorDict(dict(reward=reward), batch_size=self.batch_size),\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return next\n",
    "\n",
    "    def _reset(self, tensordict: Optional[TensorDictBase] = None) -> TensorDictBase:\n",
    "        \"\"\"(Partially) reset the environment.\n",
    "\n",
    "        For each episode which is done, takes a new sample from the dataset and resets\n",
    "        the episode.\n",
    "        \"\"\"\n",
    "\n",
    "        # If no tensordict is given, we're starting afresh\n",
    "        if tensordict is None:\n",
    "            tensordict = TensorDict(\n",
    "                dict(\n",
    "                    adjacency=torch.empty(\n",
    "                        *self.batch_size,\n",
    "                        2,\n",
    "                        self.max_num_nodes,\n",
    "                        self.max_num_nodes,\n",
    "                        device=self.device,\n",
    "                        dtype=self.int_dtype,\n",
    "                    ),\n",
    "                    x=torch.empty(\n",
    "                        *self.batch_size,\n",
    "                        2,\n",
    "                        self.max_num_nodes,\n",
    "                        self.params.max_message_rounds,\n",
    "                        device=self.device,\n",
    "                        dtype=torch.float,\n",
    "                    ),\n",
    "                    node_mask=torch.empty(\n",
    "                        *self.batch_size,\n",
    "                        2,\n",
    "                        self.max_num_nodes,\n",
    "                        device=self.device,\n",
    "                        dtype=torch.bool,\n",
    "                    ),\n",
    "                    message=torch.empty(\n",
    "                        *self.batch_size,\n",
    "                        device=self.device,\n",
    "                        dtype=torch.long,\n",
    "                    ),\n",
    "                    y=torch.empty(\n",
    "                        *self.batch_size,\n",
    "                        1,\n",
    "                        device=self.device,\n",
    "                        dtype=self.int_dtype,\n",
    "                    ),\n",
    "                    round=torch.empty(\n",
    "                        *self.batch_size,\n",
    "                        device=self.device,\n",
    "                        dtype=self.int_dtype,\n",
    "                    ),\n",
    "                    done=torch.empty(\n",
    "                        *self.batch_size,\n",
    "                        device=self.device,\n",
    "                        dtype=torch.bool,\n",
    "                    ),\n",
    "                ),\n",
    "                batch_size=self.batch_size,\n",
    "            )\n",
    "\n",
    "            new_mask = torch.ones(\n",
    "                *self.batch_size, dtype=torch.bool, device=self.device\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            new_mask = tensordict[\"done\"]\n",
    "            tensordict = tensordict.clone()\n",
    "\n",
    "        # If we don't have a data cycler yet, create one\n",
    "        if self.data_cycler is None:\n",
    "            dataloader = GeometricDataLoader(\n",
    "                self.dataset,\n",
    "                batch_size=self.num_envs,\n",
    "                follow_batch=[\"x_a\", \"x_b\"],\n",
    "                shuffle=True,\n",
    "                generator=self.rng,\n",
    "            )\n",
    "            self.data_cycler = VariableGeometricDataCycler(dataloader)\n",
    "\n",
    "        # Sample a new batch of data for the episodes that are done\n",
    "        batch = self.data_cycler.get_batch(new_mask.sum().item())\n",
    "        batch_tensordict = gi_data_to_tensordict(\n",
    "            batch, node_dim_size=self.max_num_nodes\n",
    "        )\n",
    "\n",
    "        # Copy the new data into the output\n",
    "        tensordict[\"adjacency\"][new_mask] = batch_tensordict[\"adjacency\"]\n",
    "        tensordict[\"x\"][new_mask] = torch.zeros_like(tensordict[\"x\"][new_mask])\n",
    "        tensordict[\"node_mask\"][new_mask] = batch_tensordict[\"node_mask\"]\n",
    "        tensordict[\"message\"][new_mask] = 0\n",
    "        tensordict[\"y\"][new_mask] = (batch.wl_score == -1).int().unsqueeze(-1)\n",
    "        tensordict[\"round\"][new_mask] = 0\n",
    "        tensordict[\"done\"][new_mask] = False\n",
    "\n",
    "        return tensordict\n",
    "\n",
    "    def _set_seed(self, seed: int | None):\n",
    "        self.rng.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GraphIsomorphismEnv(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] y: 0 P: (11)  0.0 V: ( 5,   1)  0.0  | [0] y: 1 P: (12)  0.0 V: (12,   0)  0.0 \n",
      "[1] y: 0 P: ( 9)  0.0 V: ( 1,   0)  1.0  | [1] y: 1 P: (14)  1.0 V: (18,   1)  1.0 \n",
      "[0] y: 0 P: (17)  0.0 V: ( 5,   1)  0.0  | [0] y: 1 P: ( 0)  0.0 V: (13,   2)  0.0 \n",
      "[1] y: 0 P: (21)  1.0 V: (11,   1)  0.0  | [1] y: 1 P: (19)  0.0 V: (16,   2)  0.0 \n",
      "[0] y: 0 P: ( 3)  0.0 V: (14,   0)  0.0  | [2] y: 1 P: (13)  0.0 V: (10,   0)  0.0 \n",
      "[1] y: 0 P: ( 3)  1.0 V: (11,   1)  0.0  | [3] y: 1 P: (17)  0.0 V: ( 7,   0)  0.0 \n",
      "[0] y: 0 P: ( 6)  0.0 V: (13,   2)  0.0  | [0] y: 1 P: (16)  0.0 V: (16,   2)  0.0 \n",
      "[1] y: 0 P: (17)  0.0 V: ( 0,   0)  1.0  | [1] y: 1 P: ( 9)  0.0 V: (15,   0)  0.0 \n",
      "[0] y: 0 P: (17)  0.0 V: (21,   1)  0.0  | [0] y: 1 P: (16)  0.0 V: (10,   2)  0.0 \n",
      "[1] y: 0 P: (17)  0.0 V: ( 2,   0)  1.0  | [1] y: 1 P: ( 0)  0.0 V: ( 3,   0)  0.0 \n",
      "[0] y: 0 P: (17)  0.0 V: (20,   1)  0.0  | [0] y: 1 P: ( 1)  0.0 V: ( 9,   1)  0.0 \n",
      "[1] y: 0 P: (12)  0.0 V: ( 0,   0)  1.0  | [1] y: 1 P: ( 6)  0.0 V: (16,   0)  0.0 \n",
      "[0] y: 1 P: (18)  0.0 V: ( 0,   0)  0.0  | [0] y: 1 P: ( 8)  0.0 V: ( 4,   1)  0.0 \n",
      "[1] y: 1 P: ( 2)  1.0 V: ( 7,   1)  1.0  | [1] y: 1 P: (15)  1.0 V: (13,   1)  1.0 \n",
      "[0] y: 0 P: (13)  0.0 V: (13,   1)  0.0  | [0] y: 0 P: (20)  0.0 V: ( 9,   2)  0.0 \n",
      "[1] y: 0 P: ( 9)  1.0 V: ( 4,   1)  0.0  | [1] y: 0 P: (20)  1.0 V: ( 7,   1)  0.0 \n",
      "[0] y: 0 P: ( 1)  0.0 V: (21,   2)  0.0  | [0] y: 0 P: (16)  0.0 V: (17,   2)  0.0 \n",
      "[1] y: 0 P: ( 3)  1.0 V: (15,   1)  0.0  | [1] y: 0 P: ( 7)  1.0 V: (12,   1)  0.0 \n",
      "[0] y: 0 P: (16)  0.0 V: (14,   1)  0.0  | [0] y: 1 P: ( 5)  0.0 V: ( 8,   2)  0.0 \n",
      "[1] y: 0 P: (19)  0.0 V: (18,   0)  1.0  | [1] y: 1 P: ( 3)  1.0 V: ( 5,   1)  1.0 \n",
      "[0] y: 0 P: ( 8)  0.0 V: ( 8,   0)  0.0  | [0] y: 0 P: ( 9)  0.0 V: (13,   1)  0.0 \n",
      "[1] y: 0 P: (10)  0.0 V: ( 1,   0)  1.0  | [1] y: 0 P: (14)  0.0 V: (21,   0)  1.0 \n",
      "[0] y: 1 P: (16)  0.0 V: ( 9,   0)  0.0  | [0] y: 0 P: (13)  0.0 V: (14,   1)  0.0 \n",
      "[1] y: 1 P: (20)  0.0 V: ( 3,   0)  0.0  | [1] y: 0 P: ( 8)  0.0 V: (13,   0)  1.0 \n",
      "[0] y: 1 P: ( 8)  0.0 V: ( 3,   2)  0.0  | [0] y: 1 P: ( 6)  0.0 V: ( 2,   2)  0.0 \n",
      "[1] y: 1 P: ( 9)  0.0 V: ( 0,   0)  0.0  | [1] y: 1 P: ( 0)  0.0 V: ( 6,   0)  0.0 \n",
      "[0] y: 0 P: ( 2)  0.0 V: (14,   1)  0.0  | [0] y: 0 P: (21)  0.0 V: ( 8,   1)  0.0 \n",
      "[1] y: 0 P: (10)  1.0 V: ( 5,   1)  0.0  | [1] y: 0 P: ( 5)  0.0 V: (10,   0)  1.0 \n",
      "[0] y: 1 P: (11)  0.0 V: ( 6,   1)  0.0  | [0] y: 1 P: (14)  0.0 V: (11,   0)  0.0 \n",
      "[1] y: 1 P: (15)  0.0 V: (12,   2)  0.0  | [1] y: 1 P: ( 4)  0.0 V: ( 2,   2)  0.0 \n",
      "[2] y: 1 P: ( 7)  0.0 V: (20,   2)  0.0  | [2] y: 1 P: ( 8)  0.0 V: (12,   0)  0.0 \n",
      "[3] y: 1 P: (21)  1.0 V: ( 5,   1)  1.0  | [3] y: 1 P: ( 1)  0.0 V: ( 8,   0)  0.0 \n",
      "[0] y: 0 P: ( 5)  0.0 V: (10,   0)  0.0  | [0] y: 1 P: (12)  0.0 V: ( 3,   1)  0.0 \n",
      "[1] y: 0 P: (11)  1.0 V: ( 4,   1)  0.0  | [1] y: 1 P: (21)  0.0 V: (20,   2)  0.0 \n",
      "[0] y: 0 P: ( 5)  0.0 V: ( 0,   2)  0.0  | [2] y: 1 P: (12)  0.0 V: (16,   2)  0.0 \n",
      "[1] y: 0 P: (17)  1.0 V: ( 9,   1)  0.0  | [3] y: 1 P: (21)  0.0 V: ( 1,   2)  0.0 \n",
      "[0] y: 1 P: (12)  0.0 V: (15,   1)  0.0  | [4] y: 1 P: (10)  0.0 V: ( 0,   0)  0.0 \n",
      "[1] y: 1 P: (14)  0.0 V: ( 6,   2)  0.0  | [5] y: 1 P: ( 1)  0.0 V: (14,   0)  0.0 \n",
      "[2] y: 1 P: ( 2)  0.0 V: ( 3,   0)  0.0  | [0] y: 1 P: ( 1)  0.0 V: ( 8,   1)  0.0 \n"
     ]
    }
   ],
   "source": [
    "def printer(env, tensordict):\n",
    "    to_print = []\n",
    "    for i in range(2):\n",
    "        to_print.append(\n",
    "            f\"[{tensordict['round'][i].item()}] \"\n",
    "            f\"y: {tensordict['y'][i].item()} \"\n",
    "            f\"P: ({tensordict['agents', 'node_selected'][i, PROVER_AGENT_NUM].item():>2}) \"\n",
    "            f\" {tensordict['next', 'agents', 'reward'][i, PROVER_AGENT_NUM].item():>2} \"\n",
    "            f\"V: ({tensordict['agents', 'node_selected'][i, VERIFIER_AGENT_NUM].item():>2}, \"\n",
    "            f\" {tensordict['agents', 'decision'][i, VERIFIER_AGENT_NUM].item():>2}) \"\n",
    "            f\" {tensordict['next', 'agents', 'reward'][i, VERIFIER_AGENT_NUM].item():>2} \"\n",
    "        )\n",
    "    print(\" | \".join(to_print))\n",
    "    # print(tensordict[\"message\"][:2, ..., :3].transpose(-1, -2))\n",
    "    # print(tensordict[\"message\"][:2, ..., :3])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = env.rollout(\n",
    "        max_steps=40,\n",
    "        callback=printer,\n",
    "        auto_cast_to_device=True,\n",
    "        break_when_any_done=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        adjacency: Tensor(shape=torch.Size([125, 40, 2, 11, 11]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        agents: TensorDict(\n",
       "            fields={\n",
       "                decision: Tensor(shape=torch.Size([125, 40, 2]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                node_selected: Tensor(shape=torch.Size([125, 40, 2]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
       "            batch_size=torch.Size([125, 40]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([125, 40]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        message: Tensor(shape=torch.Size([125, 40]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                adjacency: Tensor(shape=torch.Size([125, 40, 2, 11, 11]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                agents: TensorDict(\n",
       "                    fields={\n",
       "                        reward: Tensor(shape=torch.Size([125, 40, 2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                    batch_size=torch.Size([125, 40]),\n",
       "                    device=cpu,\n",
       "                    is_shared=False),\n",
       "                done: Tensor(shape=torch.Size([125, 40]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                message: Tensor(shape=torch.Size([125, 40]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                node_mask: Tensor(shape=torch.Size([125, 40, 2, 11]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                round: Tensor(shape=torch.Size([125, 40]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([125, 40]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                x: Tensor(shape=torch.Size([125, 40, 2, 11, 8]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "            batch_size=torch.Size([125, 40]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        node_mask: Tensor(shape=torch.Size([125, 40, 2, 11]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        round: Tensor(shape=torch.Size([125, 40]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([125, 40]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        x: Tensor(shape=torch.Size([125, 40, 2, 11, 8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        y: Tensor(shape=torch.Size([125, 40, 1]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
       "    batch_size=torch.Size([125, 40]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy and critic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphIsomorphismCombinedBody(TensorDictModuleBase):\n",
    "    in_keys = (\"round\", \"x\", \"adjacency\", \"message\", \"node_mask\")\n",
    "    out_keys = (\"round\", (\"agents\", \"node_level_repr\"), (\"agents\", \"graph_level_repr\"))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prover_body: GraphIsomorphismAgentBody,\n",
    "        verifier_body: GraphIsomorphismAgentBody,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.prover_body = prover_body\n",
    "        self.verifier_body = verifier_body\n",
    "\n",
    "    def forward(self, data: TensorDictBase) -> TensorDict:\n",
    "        round: Int[Tensor, \"batch\"] = data[\"round\"]\n",
    "\n",
    "        # Build tensordicts to feed to each agent when it's their turn\n",
    "        input_prover_dict = {}\n",
    "        input_verifier_dict = {}\n",
    "        for key in self.prover_body.in_keys:\n",
    "            if key == \"ignore_message\":\n",
    "                input_prover_dict[key] = torch.zeros_like(round, dtype=torch.bool)\n",
    "                input_verifier_dict[key] = round == 0\n",
    "            else:\n",
    "                input_prover_dict[key] = data[key]\n",
    "                input_verifier_dict[key] = data[key]\n",
    "        input_prover = TensorDict(\n",
    "            input_prover_dict,\n",
    "            batch_size=data.batch_size,\n",
    "        )\n",
    "        input_verifier = TensorDict(\n",
    "            input_verifier_dict,\n",
    "            batch_size=data.batch_size,\n",
    "        )\n",
    "\n",
    "        # Run the prover and verifier bodies\n",
    "        output_prover = self.prover_body(input_prover)\n",
    "        output_verifier = self.verifier_body(input_verifier)\n",
    "\n",
    "        # Stack the outputs\n",
    "        node_level_repr = torch.stack(\n",
    "            [output_prover[\"node_level_repr\"], output_verifier[\"node_level_repr\"]],\n",
    "            dim=-3,\n",
    "        )\n",
    "        graph_level_repr = torch.stack(\n",
    "            [output_prover[\"graph_level_repr\"], output_verifier[\"graph_level_repr\"]],\n",
    "            dim=-2,\n",
    "        )\n",
    "        if PROVER_AGENT_NUM == 1:\n",
    "            node_level_repr = node_level_repr.flip(-3)\n",
    "            graph_level_repr = graph_level_repr.flip(-2)\n",
    "\n",
    "        return data.update(\n",
    "            dict(\n",
    "                agents=dict(\n",
    "                    node_level_repr=node_level_repr,\n",
    "                    graph_level_repr=graph_level_repr,\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphIsomorphismCombinedPolicyHead(TensorDictModuleBase):\n",
    "    in_keys = ((\"agents\", \"node_level_repr\"), (\"agents\", \"graph_level_repr\"))\n",
    "    out_keys = ((\"agents\", \"node_selected_logits\"), (\"agents\", \"decision_logits\"))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prover_policy_head: GraphIsomorphismAgentPolicyHead,\n",
    "        verifier_policy_head: GraphIsomorphismAgentPolicyHead,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.prover_policy_head = prover_policy_head\n",
    "        self.verifier_policy_head = verifier_policy_head\n",
    "\n",
    "    def forward(self, head_output: TensorDictBase) -> TensorDict:\n",
    "        \"\"\"Run the prover and verifier policy heads and combine their outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tensordict : TensorDictBase\n",
    "            The input to the value heads. Should contain the keys:\n",
    "\n",
    "            - (\"agents\", \"node_level_repr\"): The node-level representation from the\n",
    "              body.\n",
    "            - (\"agents\", \"graph_level_repr\"): The node-level representation from the\n",
    "              body.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensordict: TensorDict\n",
    "            The tensordict update in place with the output of the value heads.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the policy heads to obtain the probability distributions\n",
    "        input_prover = TensorDict(\n",
    "            dict(\n",
    "                node_level_repr=head_output[\"agents\", \"node_level_repr\"][\n",
    "                    ..., PROVER_AGENT_NUM, :, :, :\n",
    "                ],\n",
    "                graph_level_repr=head_output[\"agents\", \"graph_level_repr\"][\n",
    "                    ..., PROVER_AGENT_NUM, :, :\n",
    "                ],\n",
    "            ),\n",
    "            batch_size=head_output.batch_size,\n",
    "        )\n",
    "        input_verifier = TensorDict(\n",
    "            dict(\n",
    "                node_level_repr=head_output[\"agents\", \"node_level_repr\"][\n",
    "                    ..., VERIFIER_AGENT_NUM, :, :, :\n",
    "                ],\n",
    "                graph_level_repr=head_output[\"agents\", \"graph_level_repr\"][\n",
    "                    ..., VERIFIER_AGENT_NUM, :, :\n",
    "                ],\n",
    "            ),\n",
    "            batch_size=head_output.batch_size,\n",
    "        )\n",
    "        output_prover = self.prover_policy_head(input_prover)\n",
    "        output_verifier = self.verifier_policy_head(input_verifier)\n",
    "\n",
    "        # Stack the outputs\n",
    "        node_selected_logits = torch.stack(\n",
    "            [\n",
    "                output_prover[\"node_selected_logits\"],\n",
    "                output_verifier[\"node_selected_logits\"],\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )\n",
    "        decision_logits = torch.stack(\n",
    "            [\n",
    "                torch.zeros_like(output_verifier[\"decision_logits\"]),\n",
    "                output_verifier[\"decision_logits\"],\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )\n",
    "        if PROVER_AGENT_NUM == 1:\n",
    "            node_selected_logits = node_selected_logits.flip(-2)\n",
    "            decision_logits = decision_logits.flip(-2)\n",
    "\n",
    "        return head_output.update(\n",
    "            dict(\n",
    "                agents=TensorDict(\n",
    "                    dict(\n",
    "                        node_selected_logits=node_selected_logits,\n",
    "                        decision_logits=decision_logits,\n",
    "                    ),\n",
    "                    batch_size=head_output.batch_size,\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphIsomorphismCombinedValueHead(TensorDictModuleBase):\n",
    "    in_keys = ((\"agents\", \"graph_level_repr\"),)\n",
    "    out_keys = ((\"agents\", \"value\"),)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prover_value_head: GraphIsomorphismAgentValueHead,\n",
    "        verifier_value_head: GraphIsomorphismAgentValueHead,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.prover_value_head = prover_value_head\n",
    "        self.verifier_value_head = verifier_value_head\n",
    "\n",
    "    def forward(self, head_output: TensorDictBase) -> TensorDict:\n",
    "        \"\"\"Run the prover and verifier value heads and combine their values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tensordict : TensorDictBase\n",
    "            The input to the value heads. Should contain the keys:\n",
    "\n",
    "            - (\"agents\", \"graph_level_repr\"): The node-level representation from the\n",
    "              body.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensordict: TensorDict\n",
    "            The tensordict update in place with the output of the value heads.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the policy heads to obtain the value estimates\n",
    "        input_prover = TensorDict(\n",
    "            dict(\n",
    "                graph_level_repr=head_output[\"agents\", \"graph_level_repr\"][\n",
    "                    ..., PROVER_AGENT_NUM, :, :\n",
    "                ],\n",
    "            ),\n",
    "            batch_size=head_output.batch_size,\n",
    "        )\n",
    "        input_verifier = TensorDict(\n",
    "            dict(\n",
    "                graph_level_repr=head_output[\"agents\", \"graph_level_repr\"][\n",
    "                    ..., VERIFIER_AGENT_NUM, :, :\n",
    "                ],\n",
    "            ),\n",
    "            batch_size=head_output.batch_size,\n",
    "        )\n",
    "        output_prover = self.prover_value_head(input_prover)\n",
    "        output_verifier = self.verifier_value_head(input_verifier)\n",
    "\n",
    "        # Stack the outputs\n",
    "        value = torch.stack(\n",
    "            [output_prover[\"value\"], output_verifier[\"value\"]],\n",
    "            dim=-1,\n",
    "        )\n",
    "        if PROVER_AGENT_NUM == 1:\n",
    "            value = value.flip(-2)\n",
    "\n",
    "        return head_output.update(\n",
    "            dict(\n",
    "                agents=TensorDict(\n",
    "                    dict(value=value),\n",
    "                    batch_size=head_output.batch_size,\n",
    "                )\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite categorical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeCategoricalDistribution(CompositeDistribution):\n",
    "    \"\"\"A composition of categorical distributions.\n",
    "\n",
    "    Allows specifying the parameters of the categorical distributions either as logits\n",
    "    or as probabilities.\n",
    "\n",
    "    The `log_prob` method is reimplemented with the following changes: \n",
    "    \n",
    "    - The log-probability can be stored in a different key (specified by the\n",
    "      \"log_prob_key\" parameter)\n",
    "    - It only computes stores the total log-probability, not the individual ones\n",
    "    - It doesn't reduce all non-batch dimensions in the log-probability.\n",
    "\n",
    "    Parameter names must be strings ending in \"_logits\" or \"_probs\". However, the\n",
    "    suffix-stripped can be changed by passing a key transform function or a lookup\n",
    "    table. For example, to specify the parameters of a categorical distribution over key\n",
    "    `(\"agents\", \"action\")` using logits, you can pass the following:\n",
    "    \n",
    "    >>> CompositeCategoricalDistribution(\n",
    "    ...     action_logits=..., \n",
    "    ...     key_transform=lambda x: (\"agents\", x)\n",
    "    ... )\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    **categorical_params : dict[str, Tensor]\n",
    "        The parameters of the categorical distributions. Each key is the name of a\n",
    "        categorical parameter appended with \"_logits\" or \"_probs\" and each value is a\n",
    "        Tensor containing the logits or probabilities of the categorical distribution.\n",
    "    key_transform : callable[[str], [str | tuple[str]]] | dict[str, str | tuple[str]],\n",
    "    optional\n",
    "        A function that transforms the keys of the categorical parameters. If a dict is\n",
    "        given, it is used as a lookup table. If a callable is given, it is applied to\n",
    "        each key. Defaults to the identity function.\n",
    "    log_prob_key: NestedKey, default=\"sample_log_prob\"\n",
    "        The tensordict key to use for the log-probability of the sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Get the key transform\n",
    "        try:\n",
    "            key_transform = kwargs.pop(\"key_transform\")\n",
    "            if isinstance(key_transform, dict):\n",
    "                key_transform = lambda x: key_transform[x]\n",
    "            elif not callable(key_transform):\n",
    "                raise ValueError(\"key_transform must be a callable or a dict.\")\n",
    "        except KeyError:\n",
    "            key_transform = lambda x: x\n",
    "\n",
    "        # Get the log-probability key\n",
    "        self.log_prob_key = kwargs.pop(\"log_prob_key\", \"sample_log_prob\")\n",
    "\n",
    "        batch_size = None\n",
    "        composite_params = {}\n",
    "        name_suffixes = (\"logits\", \"probs\")\n",
    "        for name, param_value in kwargs.items():\n",
    "            for name_suffix in name_suffixes:\n",
    "                if name.endswith(\"_\" + name_suffix):\n",
    "                    # Set the parameters of the categorical distribution\n",
    "                    resolved_param_name = key_transform(name[: -len(name_suffix) - 1])\n",
    "                    composite_params[resolved_param_name] = {name_suffix: param_value}\n",
    "\n",
    "                    # Make sure all the categorical parameters have the same batch size\n",
    "                    if batch_size is None:\n",
    "                        batch_size = param_value.shape[0]\n",
    "                    elif batch_size != param_value.shape[0]:\n",
    "                        raise ValueError(\n",
    "                            \"All categorical parameters must have the same batch size.\"\n",
    "                        )\n",
    "\n",
    "        composite_params_td = TensorDict(composite_params, batch_size=batch_size)\n",
    "\n",
    "        super().__init__(\n",
    "            params=composite_params_td,\n",
    "            distribution_map={key: Categorical for key in composite_params},\n",
    "        )\n",
    "\n",
    "\n",
    "    def log_prob(self, sample: TensorDictBase) -> TensorDictBase:\n",
    "        \"\"\"Computes the log probability of a sample for the composite distribution\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: TensorDictBase\n",
    "            A tensordict containing the sample\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        updated_sample: TensorDictBase\n",
    "            The sample tensordict updated with the log probability of the sample\n",
    "        \"\"\"\n",
    "        slp = 0.0\n",
    "        d = {}\n",
    "        for name, dist in self.dists.items():\n",
    "            slp += dist.log_prob(sample.get(name))\n",
    "        d[self.log_prob_key] = slp\n",
    "        sample.update(d)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        decision: Tensor(shape=torch.Size([125, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        node_selected: Tensor(shape=torch.Size([125, 2]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "    batch_size=torch.Size([125]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = CompositeCategoricalDistribution(\n",
    "    node_selected_logits=torch.randn([125, 2, 22]),\n",
    "    decision_logits=torch.randn([125, 2, 3]),\n",
    ")\n",
    "d.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating actors and value estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultInteractionMixin(ProbabilisticTensorDictSequential):\n",
    "    \"\"\"A mixin for accessing the last module's default interaction type.\n",
    "\n",
    "    Allows access to 'default_interaction_mode' and 'default_interaction_type'\n",
    "    attributes for the last module in the sequential.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def default_interaction_mode(self) -> str:\n",
    "        \"\"\"The default interaction mode of the last module in the sequential.\"\"\"\n",
    "        return self[-1].default_interaction_mode\n",
    "\n",
    "    @property\n",
    "    def default_interaction_type(self) -> InteractionType:\n",
    "        \"\"\"The default interaction type of the last module in the sequential.\"\"\"\n",
    "        return self[-1].default_interaction_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticActorDefaultInteractionType(\n",
    "    ProbabilisticActor, DefaultInteractionMixin\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prover_body = GraphIsomorphismAgentBody(params, \"prover\", device=device)\n",
    "verifier_body = GraphIsomorphismAgentBody(params, \"verifier\", device=device)\n",
    "prover_policy_head = GraphIsomorphismAgentPolicyHead(params, \"prover\", device=device)\n",
    "verifier_policy_head = GraphIsomorphismAgentPolicyHead(\n",
    "    params, \"verifier\", device=device\n",
    ")\n",
    "prover_value_head = GraphIsomorphismAgentValueHead(params, \"prover\", device=device)\n",
    "verifier_value_head = GraphIsomorphismAgentValueHead(\n",
    "    params, \"verifier\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = GraphIsomorphismCombinedBody(prover_body, verifier_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_head = ProbabilisticActorDefaultInteractionType(\n",
    "    GraphIsomorphismCombinedPolicyHead(prover_policy_head, verifier_policy_head),\n",
    "    spec=env.action_spec,\n",
    "    distribution_class=CompositeCategoricalDistribution,\n",
    "    distribution_kwargs=dict(\n",
    "        key_transform=lambda x: (\"agents\", x),\n",
    "        log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    "    ),\n",
    "    in_keys=dict(\n",
    "        node_selected_logits=(\"agents\", \"node_selected_logits\"),\n",
    "        decision_logits=(\"agents\", \"decision_logits\"),\n",
    "    ),\n",
    "    out_keys=env.action_keys,\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_prob_module = SafeProbabilisticModule(\n",
    "#     spec=env.action_spec,\n",
    "#     distribution_class=CompositeCategoricalDistribution,\n",
    "#     in_keys=dict(\n",
    "#         node_selected_logits=(\"agents\", \"node_selected_logits\"),\n",
    "#         decision_logits=(\"agents\", \"decision_logits\"),\n",
    "#     ),\n",
    "#     out_keys=env.action_keys,\n",
    "#     return_log_prob=True,\n",
    "#     log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_head = GraphIsomorphismCombinedValueHead(\n",
    "    prover_value_head, verifier_value_head\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = ActorValueOperator(body, policy_head, value_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = full_model.get_policy_operator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        adjacency: Tensor(shape=torch.Size([125, 2, 11, 11]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        agents: TensorDict(\n",
       "            fields={\n",
       "                decision: Tensor(shape=torch.Size([125, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                decision_logits: Tensor(shape=torch.Size([125, 2, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                graph_level_repr: Tensor(shape=torch.Size([125, 2, 2, 16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                node_level_repr: Tensor(shape=torch.Size([125, 2, 2, 11, 16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                node_selected: Tensor(shape=torch.Size([125, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                node_selected_logits: Tensor(shape=torch.Size([125, 2, 22]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                sample_log_prob: Tensor(shape=torch.Size([125, 2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "            batch_size=torch.Size([125]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([125]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        message: Tensor(shape=torch.Size([125]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        node_mask: Tensor(shape=torch.Size([125, 2, 11]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        round: Tensor(shape=torch.Size([125]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([125]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        x: Tensor(shape=torch.Size([125, 2, 11, 8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        y: Tensor(shape=torch.Size([125, 1]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
       "    batch_size=torch.Size([125]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        adjacency: Tensor(shape=torch.Size([125, 2, 11, 11]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        agents: TensorDict(\n",
       "            fields={\n",
       "                graph_level_repr: Tensor(shape=torch.Size([125, 2, 2, 16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                node_level_repr: Tensor(shape=torch.Size([125, 2, 2, 11, 16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                value: Tensor(shape=torch.Size([125, 2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "            batch_size=torch.Size([125]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([125]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        message: Tensor(shape=torch.Size([125]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        node_mask: Tensor(shape=torch.Size([125, 2, 11]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        round: Tensor(shape=torch.Size([125]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([125]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        x: Tensor(shape=torch.Size([125, 2, 11, 8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        y: Tensor(shape=torch.Size([125, 1]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
       "    batch_size=torch.Size([125]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.get_value_operator()(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = ProbabilisticTensorDictSequential(full_model, policy_head, policy_prob_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReplayBuffer(storage=<torchrl.data.replay_buffers.storages.LazyTensorStorage object at 0x7f6f500f1410>, sampler=<torchrl.data.replay_buffers.samplers.SamplerWithoutReplacement object at 0x7f6f5b2e97d0>, writer=<torchrl.data.replay_buffers.writers.RoundRobinWriter object at 0x7f6f500f0450>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        params.ppo.frames_per_batch, device=device\n",
    "    ),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=params.ppo.minibatch_size,\n",
    ")\n",
    "replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=params.ppo.frames_per_batch,\n",
    "    total_frames=params.ppo.frames_per_batch * params.ppo.num_iterations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLossMultipleActions(\n",
    "    actor=full_model.get_policy_operator(),\n",
    "    critic=full_model.get_value_operator(),\n",
    "    clip_epsilon=params.ppo.clip_epsilon,\n",
    "    entropy_coef=params.ppo.entropy_eps,\n",
    "    normalize_advantage=False,\n",
    ")\n",
    "loss_module.set_keys(\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_keys,\n",
    "    sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "    value=(\"agents\", \"value\"),\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=params.ppo.gamma, lmbda=params.ppo.lmbda\n",
    ")\n",
    "gae = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), params.ppo.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_head.prover_value_head.value_mlp.module[0].__dict__[\"_is_stateless\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rearrange('... pair d_in -> ... (pair d_in)')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_head.prover_value_head.value_mlp.module[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_hooks_always_called', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'pattern', 'axes_lengths', '_multirecipe', '_axes_lengths', '_functionalized', '_decorated_funs', 'forward', '_is_stateless'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_head.prover_value_head.value_mlp.module[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_head.prover_value_head.value_mlp.module[0].__dict__[\"_test\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = 0:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/sam/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torch/nn/functional.py:5504: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_attention_math. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params\u001b[38;5;241m.\u001b[39mppo\u001b[38;5;241m.\u001b[39mframes_per_batch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m params\u001b[38;5;241m.\u001b[39mppo\u001b[38;5;241m.\u001b[39mminibatch_size):\n\u001b[1;32m     31\u001b[0m     subdata \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 32\u001b[0m     loss_vals \u001b[38;5;241m=\u001b[39m \u001b[43mloss_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     35\u001b[0m         loss_vals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_objective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;241m+\u001b[39m loss_vals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_critic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;241m+\u001b[39m loss_vals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_entropy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     40\u001b[0m     loss_value\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/tensordict/nn/common.py:281\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(out[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dest)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torchrl/objectives/ppo.py:654\u001b[0m, in \u001b[0;36mClipPPOLoss.forward\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    651\u001b[0m     scale \u001b[38;5;241m=\u001b[39m advantage\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m1e-6\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    652\u001b[0m     advantage \u001b[38;5;241m=\u001b[39m (advantage \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale\n\u001b[0;32m--> 654\u001b[0m log_weight, dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# ESS for logging\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;66;03m# In theory, ESS should be computed on particles sampled from the same source. Here we sample according\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;66;03m# to different, unrelated trajectories, which is not standard. Still it can give a idea of the dispersion\u001b[39;00m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;66;03m# of the weights.\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/Projects/PVG Experiments/pvg/utils/torchrl_objectives.py:161\u001b[0m, in \u001b[0;36mPPOLossMultipleActions._log_weight\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensordict stored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactions_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         )\n\u001b[1;32m    159\u001b[0m action_tensordict \u001b[38;5;241m=\u001b[39m TensorDict(actions, batch_size\u001b[38;5;241m=\u001b[39mactions_batch_size)\n\u001b[0;32m--> 161\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, CompositeDistribution):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActor must return a CompositeDistribution to work with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdist\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/tensordict/nn/probabilistic.py:524\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.get_dist\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dist\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    519\u001b[0m     tensordict: TensorDictBase,\n\u001b[1;32m    520\u001b[0m     tensordict_out: TensorDictBase \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m D\u001b[38;5;241m.\u001b[39mDistribution:\n\u001b[1;32m    523\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the distribution that results from passing the input tensordict through the sequence, and then using the resulting parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dist_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_dist_from_params(tensordict_out)\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/tensordict/nn/probabilistic.py:515\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.get_dist_params\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdefault_interaction_type\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_interaction_type(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:573\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# get the previous params, and tell the submodules not to look for params anymore\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m old_params \u001b[38;5;241m=\u001b[39m \u001b[43m_assign_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_stateless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_old_tensordict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    577\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), fun_name)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:650\u001b[0m, in \u001b[0;36m_assign_params\u001b[0;34m(module, params, make_stateless, return_old_tensordict)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assign_params\u001b[39m(\n\u001b[1;32m    644\u001b[0m     module: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    645\u001b[0m     params: TensorDict,\n\u001b[1;32m    646\u001b[0m     make_stateless: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    647\u001b[0m     return_old_tensordict: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    648\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorDict \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_swap_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_stateless\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_old_tensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:390\u001b[0m, in \u001b[0;36m_swap_state\u001b[0;34m(model, tensordict, is_stateless, return_old_tensordict, old_tensordict)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_tensor_collection(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    389\u001b[0m     _old_value \u001b[38;5;241m=\u001b[39m old_tensordict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 390\u001b[0m     _old_value \u001b[38;5;241m=\u001b[39m \u001b[43m_swap_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_modules\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_stateless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_stateless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mold_tensordict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_old_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_old_tensordict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_old_tensordict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     old_tensordict_dict[key] \u001b[38;5;241m=\u001b[39m _old_value\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/pvg-experiments/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:391\u001b[0m, in \u001b[0;36m_swap_state\u001b[0;34m(model, tensordict, is_stateless, return_old_tensordict, old_tensordict)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_tensor_collection(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    389\u001b[0m     _old_value \u001b[38;5;241m=\u001b[39m old_tensordict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    390\u001b[0m     _old_value \u001b[38;5;241m=\u001b[39m _swap_state(\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_modules\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    392\u001b[0m         value,\n\u001b[1;32m    393\u001b[0m         is_stateless\u001b[38;5;241m=\u001b[39mis_stateless,\n\u001b[1;32m    394\u001b[0m         old_tensordict\u001b[38;5;241m=\u001b[39m_old_value,\n\u001b[1;32m    395\u001b[0m         return_old_tensordict\u001b[38;5;241m=\u001b[39mreturn_old_tensordict,\n\u001b[1;32m    396\u001b[0m     )\n\u001b[1;32m    397\u001b[0m     old_tensordict_dict[key] \u001b[38;5;241m=\u001b[39m _old_value\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '2'"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=params.ppo.num_iterations, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for tensordict_data in collector:\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gae(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_params,\n",
    "            target_params=loss_module.target_critic_params,\n",
    "        )  # Compute GAE and add it to the data\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    for _ in range(params.ppo.num_epochs):\n",
    "        for _ in range(params.ppo.frames_per_batch // params.ppo.minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), params.ppo.max_grad_norm\n",
    "            )  # Optional\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvg-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
