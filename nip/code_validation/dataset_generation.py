"""Module for generating datasets used in code validation systems.

A code validation dataset is generated by taking the APPS dataset and modifying
solutions to create buggy solutions, using language models.

A `CodeValidationDatasetConfig` class is provided to configure the generation of buggy
solutions for a dataset of problems. The `generate_and_save_cv_dataset` function is used
to generate buggy solutions for a given dataset of problems and save the combined
dataset to disk.
"""

from typing import Optional, Literal, Any
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime, timedelta
import json
import os
import sys
from math import floor
import re
import textdistance
import requests
import multiprocessing
from multiprocessing.managers import SyncManager, ListProxy
import importlib.resources
from logging import getLogger
from string import Template

from tqdm import tqdm

import datasets

from openai import OpenAI
from openai.types.chat.chat_completion import Choice as OpenAIChoice

from nip.constants import CV_DATA_DIR, HF_BUGGY_APPS_REPO
from nip.utils.apps_metric import check_correctness
from nip.utils.env import load_env_once

ORDINALS = [
    "zeroth",
    "first",
    "second",
    "third",
    "fourth",
    "fifth",
    "sixth",
    "seventh",
    "eighth",
    "ninth",
    "tenth",
]


@dataclass
class CodeValidationDatasetConfig:
    """A configuration class for generating datasets used in code validation systems.

    Attributes
    ----------
    model : str
        The model to be used, default is "openai/gpt-4o-mini".
    difficulties : list of str
        List of difficulty levels, default is ["interview", "competition",
        "introductory"].
    split : {'train', 'test'}, optional
        The data split, default is None.
    fraction_to_modify : float
        Fraction of data to modify, default is 0.5.
    max_modifications : int
        Maximum number of modifications, default is 1.
    num_data : int, optional
        Number of data points per split per difficulty level, default is 10000.
    num_problematic_inputs : int
        Number of problematic inputs to request, default is 0.
    system_prompt : str, optional
        System prompt for generating incorrect solutions, default is None.
    max_attempts : int
        Maximum number of attempts to generate a valid buggy solution, default is 10.
    local_dir : str
        Local directory for data storage.
    pull_repo : str | None
        Repository to pull data from, default is the value of
        `constants.HF_BUGGY_APPS_REPO`.
    push_repo : str | None
        Repository to push data to, default is the value of
        `constants.HF_BUGGY_APPS_REPO`.
    save_after : int, optional
        Number of operations after which to save data, default is 10.

    Methods
    -------
    __post_init__():
        Initializes the system prompt based on the number of problematic inputs.
    """

    model: str = "openai/gpt-4o-mini"
    difficulties: list[str] = field(
        default_factory=lambda: ["interview", "competition", "introductory"]
    )
    split: Optional[Literal["train", "test"]] = None
    fraction_to_modify: float = 0.5
    max_modifications: int = 1
    num_data: Optional[int] = 10000  # per split per difficulty level
    num_problematic_inputs: int = 0
    system_prompt: Optional[str] = None
    max_attempts: int = 10
    local_dir: str = CV_DATA_DIR
    pull_repo: Optional[str] = HF_BUGGY_APPS_REPO
    push_repo: Optional[str] = HF_BUGGY_APPS_REPO
    save_after: Optional[int] = 10

    def __post_init__(self):
        """Set the system prompt based on the number of problematic inputs.

        This method sets the `system_prompt` attribute based on the value of
        `num_problematic_inputs`. If `system_prompt` is already provided, it does
        nothing. Otherwise, it generates a prompt for generating incorrect solutions and
        problematic inputs for a code validation system.

        - If `num_problematic_inputs` is less than 0 or greater than 10, it raises a
          ValueError.
        - If `num_problematic_inputs` is 0, it generates a prompt for modifying a
          solution without providing problematic inputs.
        - If `num_problematic_inputs` is between 1 and 10, it generates a prompt for
          modifying a solution and provides placeholders for the specified number of
          problematic inputs.

        Raises
        ------
        ValueError
            If `num_problematic_inputs` is not between 0 and 10.
        """

        if self.system_prompt is None:

            prompt_template_traversable = importlib.resources.files(
                "nip.code_validation.prompt_templates.dataset_generation"
            )

            if not 0 <= self.num_problematic_inputs <= 10:
                raise ValueError("num_problematic_inputs must be between 0 and 10")

            elif self.num_problematic_inputs == 0:
                template = Template(
                    prompt_template_traversable.joinpath(
                        "no_problematic_inputs"
                    ).read_text()
                )
                self.system_prompt = template.substitute()

            else:
                problematic_inputs = "\n\n".join(
                    [
                        f"PROBLEMATIC INPUT {i+1}:\n<your {ORDINALS[i]} problematic "
                        f"input here>"
                        for i in range(self.num_problematic_inputs)
                    ]
                )

                template = Template(
                    prompt_template_traversable.joinpath(
                        "with_problematic_inputs"
                    ).read_text()
                )
                self.system_prompt = template.substitute(
                    problematic_inputs=problematic_inputs
                )


def generate_and_save_cv_dataset(
    config: CodeValidationDatasetConfig | dict,
    manager: Optional[SyncManager] = None,
):
    """Generate a code validation dataset and save it to disk.

    This function generates buggy solutions for a given dataset of problems and saves
    the combined dataset to disk. It uses language models to generate buggy solutions
    for a fraction of the solutions provided in the dataset.

    Parameters
    ----------
    config : CodeValidationDatasetConfig | dict
        A configuration object or dictionary for generating the dataset.
    manager : SyncManager, optional
        A multiprocessing manager to handle shared memory, If None, a new manager is
        created.

    Raises
    ------
    ValueError
        If the number of buggy solutions generated is 0.
    """

    logger = getLogger(__name__)

    load_env_once()
    hugging_face_token = os.getenv("HF_TOKEN")

    if isinstance(config, dict):
        config = CodeValidationDatasetConfig(**config)

    # Create process manager in case we get stuck on any of the problems
    if manager is None:
        manager = multiprocessing.Manager()

    process_results = manager.list()

    splits = ["train", "test"] if config.split is None else [config.split]

    # Load existing data
    data = datasets.load_dataset("codeparrot/apps", trust_remote_code=True)
    buggy_data = _load_cv_dataset(config, splits)

    start_time = datetime.now()

    for split in splits:

        # We keep track of which problems we have had trouble generating buggy data for
        # so we can skip them in the future
        problematic_problems_filename = os.path.join(
            config.local_dir, f"problematic_{split}_problems.csv"
        )
        with open(problematic_problems_filename, mode="r") as file:
            line = file.readline().strip()
            problematic_problems = [
                int(value) for value in line.split(",") if value != ""
            ]

        for difficulty in config.difficulties:

            data_slice = (
                data[split]
                .filter(lambda x: x["difficulty"] == difficulty)
                .sort("problem_id")
            )
            buggy_data_slice = buggy_data.filter(
                lambda x: x["difficulty"] == difficulty and x["apps_split"] == split
            ).sort("apps_problem_id")

            num_buggy_data_to_generate = max(
                min(config.num_data, len(data_slice)) - len(buggy_data_slice), 0
            )
            num_data_added = 0

            logger.info(
                f"Generating {num_buggy_data_to_generate} buggy data for the "
                f"{difficulty!r} problems in the {split!r} split"
            )

            new_data = _create_empty_cv_dataset()

            for datum in tqdm(data_slice, colour="green"):

                datum: dict[str, Any]

                if (
                    datum["solutions"] == ""
                    or int(datum["problem_id"]) in problematic_problems
                ):
                    continue

                # The "solutions" field is a JSON string containing a list of solutions
                solutions: list[str] = json.loads(datum["solutions"])

                # Check if we've already generated the buggy data or if there is only
                # one solution given
                if (
                    len(buggy_data_slice) >= num_buggy_data_to_generate
                    or num_data_added >= num_buggy_data_to_generate
                ):
                    break
                elif (
                    datum["problem_id"] in buggy_data_slice["apps_problem_id"]
                    or len(solutions) <= 1
                ):
                    continue

                buggy_datum = {
                    "apps_split": split,
                    "apps_problem_id": datum["problem_id"],
                    "difficulty": difficulty,
                    "question": datum["question"],
                    "solutions": [],
                    "buggy_solutions": [],
                }

                # Generate new buggy solutions
                timeout = min(20 + (5 * len(solutions)), 120)
                process = multiprocessing.Process(
                    target=_generate_buggy_solutions,
                    args=(
                        process_results,
                        datum,
                        config.model,
                        config.system_prompt,
                        config.max_modifications,
                        config.max_attempts,
                    ),
                )
                process.start()
                process.join(timeout)
                if process.is_alive():
                    process.terminate()
                    process.join()
                    logger.warning(
                        f"Generating buggy solutions for problem {datum['problem_id']} "
                        f"({split!r}) timed out after {timeout} seconds"
                    )
                    buggy_solutions = [None]
                else:
                    buggy_solutions = process_results[-1]

                # If we didn't manage to generate any buggy solutions, we make a note of
                # this problem so we can skip it in the future
                num_buggy_solutions_generated = len(
                    buggy_solutions
                ) - buggy_solutions.count(None)
                if num_buggy_solutions_generated == 0:
                    with open(problematic_problems_filename, "a") as f:
                        f.write(f"{datum['problem_id']},")
                    continue

                solutions_added = 0
                num_checks = len(json.loads(datum["input_output"])["inputs"])
                for solution_id, (solution, buggy_solution) in enumerate(
                    zip(solutions, buggy_solutions)
                ):
                    if buggy_solution is None:
                        if solutions_added < num_buggy_solutions_generated:
                            solution = {
                                "apps_solution_number": solution_id,
                                "solution": solution,
                                "levenshtein_distance": {"normalised": 0.0, "raw": 0},
                                "checks": {"pass": num_checks, "fail": 0, "error": 0},
                                "generation_attempts": 0,
                            }
                            buggy_datum["solutions"].append(solution)
                            solutions_added += 1
                    else:
                        buggy_datum["buggy_solutions"].append(buggy_solution)

                new_data = new_data.add_item(buggy_datum)
                num_data_added += 1

                # Occasionally save if required
                if config.save_after is None:
                    continue
                elif num_data_added % config.save_after == 0 and num_data_added > 0:
                    # buggy_data.to_json(os.path.join(config.local_dir, s, f"{d}.jsonl"))
                    buggy_data = _load_cv_dataset(config, splits)
                    combined_buggy_data = datasets.concatenate_datasets(
                        [buggy_data, new_data]
                    )
                    new_data = _create_empty_cv_dataset()
                    combined_buggy_data.save_to_disk(
                        os.path.join(config.local_dir, "code_validation.data")
                    )
                    if config.push_repo is not None:
                        combined_buggy_data.push_to_hub(
                            config.push_repo, token=config.token
                        )

            # Calculate the elapsed time, rounding microseconds down
            elapsed_time = datetime.now() - start_time
            elapsed_time = timedelta(
                days=elapsed_time.days, seconds=elapsed_time.seconds
            )
            commit_message = (
                f"Added {num_data_added} buggy data overall in {elapsed_time} for "
                f"the {difficulty} problems in the {split!r} split"
            )
            logger.info(commit_message)

            # Save data
            if num_data_added > 0:
                # buggy_data.to_json(os.path.join(config.local_dir, s, f"{d}.jsonl"))
                buggy_data = _load_cv_dataset(config, splits)
                combined_buggy_data = datasets.concatenate_datasets(
                    [buggy_data, new_data]
                )
                new_data = _create_empty_cv_dataset()
                combined_buggy_data.save_to_disk(
                    os.path.join(config.local_dir, "code_validation.data")
                )
                if config.push_repo is not None:
                    combined_buggy_data.push_to_hub(
                        config.push_repo,
                        commit_message=commit_message,
                        token=hugging_face_token,
                    )


def _generate_buggy_solutions(
    results: ListProxy[list[str | None]],
    datum: list[str, Any],
    model: str,
    system_prompt: str,
    max_modifications: int,
    max_attempts: Optional[int] = None,
) -> ListProxy[list[str | None]]:
    """Generate buggy solutions for a given datum and append them to the result list.

    If an exception is raised during the process, a list containing None is appended to
    the result list.

    Parameters
    ----------
    results : ListProxy[list[str | None]]
        A list proxy to store the buggy solutions.
    datum : list[str, Any]
        The corresponding datum from the original APPS dataset.
    model : str
        The model to use for generating buggy solutions.
    system_prompt : str
        The system prompt to provide context to the model.
    max_modifications : int
        The maximum number of solutions to modify.
    max_attempts : int, optional
        The maximum number of attempts to generate a valid buggy solution, default is
        None.

    Returns
    -------
    results : ListProxy[list[str | None]]
        The list proxy with the buggy solutions appended.
    """

    try:
        buggy_solutions = _try_generate_buggy_solutions(
            datum=datum,
            model=model,
            system_prompt=system_prompt,
            max_modifications=max_modifications,
            max_attempts=max_attempts,
        )
        return results.append(buggy_solutions)
    except Exception:
        return results.append([None])


def _try_generate_buggy_solutions(
    datum: dict,
    model: str,
    system_prompt: str,
    max_modifications: int,
    fraction_to_modify: float = 0.5,
    max_attempts: int = 10,
    multiple_completions: bool = True,
    existing_buggy_solutions: Optional[list[dict]] = None,
) -> list[str | None]:
    """Generate buggy solutions by modifying a fraction of the provided solutions.

    Parameters
    ----------
    datum : dict
        The corresponding datum from the original APPS dataset.
    model : str
        The model to use for generating buggy solutions.
    system_prompt : str
        The system prompt to provide context to the model.
    fraction_to_modify : float
        The fraction of solutions to modify.
    max_modifications : int
        The maximum number of solutions to modify.
    max_attempts : int, optional
        The maximum number of attempts to generate a valid buggy solution. Defaults to
        10.
    multiple_completions : bool, optional
        If True, generate multiple completions for each prompt. Defaults to True.
    existing_buggy_solutions : Optional[list[dict]], optional
        A list of existing buggy solutions to update or extend. Defaults to None.

    Returns
    -------
    list[str | None]
        A list of buggy solutions or None if a valid buggy solution could not be
        generated.
    """

    logger = getLogger(__name__)

    load_env_once()
    openrouter_api_key = os.getenv("OPENROUTER_API_KEY")

    logger.debug(f"Problem: {datum['problem_id']}")

    if existing_buggy_solutions is None:
        buggy_solutions = [None] * len(json.loads(datum["solutions"]))

    solutions = json.loads(datum["solutions"])
    if len(solutions) != len(buggy_solutions):
        raise ValueError(
            f"Both lists of solutions must have the same length. Got {len(solutions)} "
            f"solutions and {len(buggy_solutions)} buggy solutions."
        )

    user_prompt_template = Template(
        "QUESTION:\n\n\n$question\n\nSOLUTION:\n\n\n$solution"
    )

    num_to_modify = min(floor(fraction_to_modify * len(solutions)), max_modifications)
    num_modified = 0

    for solution_id, solution in enumerate(solutions):

        if buggy_solutions[solution_id] is not None:
            continue

        # Check that original solution is valid
        if not all(check_correctness(datum, solution)):
            continue

        user_prompt = user_prompt_template.substitute(
            question=datum["question"], solution=solution
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        valid_buggy_solution = False
        attempts = 0

        logger.debug(f"Solution: {solution_id}/{len(solutions)}")

        while not valid_buggy_solution and attempts < max_attempts:

            logger.debug(f"Attempt: [{attempts+1}/{max_attempts}]")

            if multiple_completions:
                model_outputs = _get_openrouter_response(
                    model, messages, openrouter_api_key, num_responses=max_attempts
                )
                model_outputs = [output["message"] for output in model_outputs]
            else:
                model_outputs = [
                    _get_openrouter_response(model, messages, openrouter_api_key)[0][
                        "message"
                    ]
                ]

            logger.debug("Model output(s) received")

            for model_output in model_outputs:

                attempts += 1

                buggy_solution, problematic_inputs, flag_unsafe = (
                    _extract_code_and_input(model_output)
                )

                logger.debug("Answer extracted")

                if flag_unsafe:
                    logger.debug("Unsafe solution generated")
                    continue

                valid_buggy_solution, input_output_checks, _ = _test_buggy_solution(
                    buggy_solution, solution, problematic_inputs, datum
                )

                if valid_buggy_solution:
                    break

        if valid_buggy_solution:
            logger.debug(f"Valid buggy solution generated in {attempts} attempts")

            # Calculate the Levenshtein distance
            levenshtein_distance = textdistance.levenshtein(solution, buggy_solution)
            normalised_levenstein_distance = levenshtein_distance / max(
                len(solution), len(buggy_solution)
            )
            levenshtein_distance = {
                "normalised": normalised_levenstein_distance,
                "raw": levenshtein_distance,
            }

            passed = input_output_checks.count(True)
            failed = input_output_checks.count(False)

            buggy_solutions[solution_id] = {
                "apps_solution_number": solution_id,
                "solution": buggy_solution,
                "levenshtein_distance": levenshtein_distance,
                "checks": {
                    "pass": passed,
                    "fail": failed,
                    "error": len(input_output_checks) - passed - failed,
                },
                "generation_attempts": attempts,
            }
            num_modified += 1

            if num_modified >= num_to_modify:
                break

        else:
            logger.warning(
                f"Failed to generate buggy solution for solution {solution_id}"
            )
            buggy_solutions[solution_id] = None

    return buggy_solutions


def _get_openrouter_response(
    model: str,
    messages: list[dict[Literal["role", "content"], str]],
    temperature: float = 1.0,
    get_log_probs: bool = False,
    get_top_logprobs: bool = None,
    num_responses: int = 1,
    force_multiple_generations: bool = False,
) -> list[dict[Literal["message", "log_probs", "top_logprobs"], Any]]:
    """Send a POST request to the OpenRouter API to get responses from a chat model.

    Note that this function calls the OpenAI API instead when it can.

    Parameters
    ----------
    model : str
        The name of the chat model to use.
    messages : list
        A list of dictionaries representing the chat messages. Each dictionary should
        have a "role" key with the value "user" or "assistant", and a "content" key with
        the content of the message.
    temperature : float, default=1.0
        The sampling temperature to use when generating completions.
    get_log_probs : bool, default=False
        Whether to return the log probabilities of the tokens in the completion.
    get_top_logprobs : bool, default=None
        Whether to return the top log probabilities of the tokens in the completion.
    num_responses : int, default=1
        The number of completions to generate.
    force_multiple_generations : bool, default=False
        If True, we force multiple generations by making multiple requests to the
        OpenRouter API.

    Returns
    -------
    responses : list[dict[Literal["message", "log_probs", "top_logprobs"], Any]]
        The response object returned by the API.

    Raises
    ------
    requests.exceptions.RequestException
        If there was an error sending the request.
    """

    load_env_once()
    api_key = os.getenv("OPENROUTER_API_KEY")

    responses = []

    # Crazily, the openrouter API doesn't support multiple completions in a single
    # request, so we have to make multiple requests.
    if "openai" not in model or "o1" in model or force_multiple_generations:
        completions = []
        for _ in range(num_responses):
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={"Authorization": f"Bearer {api_key}"},
                data=json.dumps(
                    {
                        "model": model,
                        "messages": messages,
                        "temperature": temperature,
                        "logprobs": get_log_probs,
                        "top_logprobs": get_top_logprobs,
                    }
                ),
            )
            completions.append(response.json()["choices"][0])

    else:
        completions = _get_openai_response(
            model.split("/")[-1],
            messages,
            temperature=temperature,
            log_probs=get_log_probs,
            top_logprobs=get_top_logprobs,
            num_responses=num_responses,
        )
        completions = [choice.model_dump() for choice in completions]

    for completion in completions:

        response_dict = {}
        response_dict["message"] = completion["message"]["content"]

        has_log_probs = "logprobs" in completion and completion["logprobs"] is not None

        if get_log_probs:
            if not has_log_probs:
                raise RuntimeError(
                    "Log probabilities were requested but not returned by the model."
                )
            response_dict["log_probs"] = [
                {key: content[key] for key in ["token", "logprob"]}
                for content in completion["logprobs"]["content"]
            ]
        else:
            response_dict["log_probs"] = None

        if get_top_logprobs:
            if not has_log_probs:
                raise RuntimeError(
                    "Top log probabilities were requested but not returned by the "
                    "model."
                )
            response_dict["top_logprobs"] = [
                [
                    {key: position[key] for key in ["token", "logprob"]}
                    for position in content["top_logprobs"]
                ]
                for content in completion["logprobs"]["content"]
            ]
        else:
            response_dict["top_logprobs"] = None

        responses.append(response_dict)

    return responses


def _get_openai_response(
    model: str,
    messages: list[dict[Literal["role", "content"], str]],
    temperature: float = 1.0,
    log_probs: bool = False,
    top_logprobs: bool = None,
    num_responses: int = 1,
) -> list[OpenAIChoice]:
    """Get completions from the OpenAI API for a chat model.

    Parameters
    ----------
    model : str
        The name of the chat model to use.
    messages : list[dict[Literal["role", "content"], str]]
        A list of dictionaries representing the chat messages. Each dictionary should
        have a "role" key with the value "user" or "assistant", and a "content" key with
        the content of the message.
    temperature : float, default=1.0
        The sampling temperature to use when generating completions.
    log_probs : bool, default=False
        Whether to return the log probabilities of the tokens in the completion.
    top_logprobs : bool, default=None
        Whether to return the top log probabilities of the tokens in the completion.
    num_responses : int, default=1
        The number of completions to generate.

    Returns
    -------
    completions : list[OpenAIChoice]
        A list of completions returned by the API.
    """

    load_env_once()
    api_key = os.getenv("OPENAI_API_KEY")

    client = OpenAI(api_key=api_key)

    return client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        logprobs=log_probs,
        top_logprobs=top_logprobs,
        n=num_responses,
    ).choices


def _extract_code_and_input(model_output: str) -> tuple[str | None, list[str], bool]:
    """Extract the modified solution and problematic inputs from the model output.

    This function uses regular expressions to parse the model output and extract:

    1. Problematic inputs.
    2. The modified solution.
    3. A safety check to ensure the modified solution does not contain dangerous
       functions.

    Parameters
    ----------
    model_output : str
        The output from the model containing problematic inputs and the modified
        solution.

    Returns
    -------
    modified_solution : str | None
        The modified solution extracted from the model output, or None if not found.
    problematic_inputs : list[str]
        A list of problematic inputs extracted from the model output.
    flag_unsafe : bool
        A flag indicating if the modified solution contains dangerous functions, and is
        therefore unsafe.
    """

    # Extract the problematic input using regex
    pattern = re.compile(
        r"PROBLEMATIC INPUT \d+:\n(.*?)(?=\nPROBLEMATIC INPUT \d+:|\Z)", re.DOTALL
    )
    problematic_inputs = [
        input.replace("```", "").strip() for input in pattern.findall(model_output)
    ]

    # Extract the modified solution using regex
    if problematic_inputs != []:
        pattern = re.compile(
            r"MODIFIED SOLUTION:\s*(.*?)\s*PROBLEMATIC INPUT 1:", re.DOTALL
        )
    else:
        pattern = re.compile(r"MODIFIED SOLUTION:\s*(.*)", re.DOTALL)
    match = pattern.search(model_output)
    if match:
        modified_solution = (
            match.group(1).replace("python", "").replace("```", "").strip()
        )
    else:
        modified_solution = None

    # Check for dangerous functions
    dangerous_functions = [
        "exit",
        "quit",
        "kill",
        "system",
        "putenv",
        "remove",
        "removedirs",
        "rmdir",
        "fchdir",
        "setuid",
        "fork",
        "forkpty",
        "killpg",
        "rename",
        "renames",
        "truncate",
        "replace",
        "unlink",
        "fchmod",
        "fchown",
        "chmod",
        "chown",
        "chroot",
        "lchflags",
        "lchmod",
        "lchown",
        "getcwd",
        "chdir",
        "rmtree",
        "move",
        "chown",
        "Popen",
        "__builtins__",
        "ipdb",
        "joblib",
        "resource",
        "psutil",
        "tkinter",
    ]
    flag_unsafe = any(func in modified_solution for func in dangerous_functions)

    return modified_solution, problematic_inputs, flag_unsafe


def _test_buggy_solution(
    buggy_solution: str,
    solution: str,
    problematic_inputs: list[str],
    datum: dict,
    ignore_invalid_outputs: bool = False,
) -> tuple[
    bool,
    list,
    Optional[list[dict[Literal["input", "output", "buggy_output"], str]]],
]:
    """Test a buggy solution against a correct solution using provided inputs and datum.

    Parameters
    ----------
    buggy_solution : str
        The buggy solution code to be tested.
    solution : str
        The correct solution code for comparison.
    problematic_inputs : list of str
        A list (possibly empty) of inputs that are predicted to cause issues.
    datum : dict
        The corresponding datum from the original APPS dataset.
    ignore_invalid_outputs : bool, optional
        If True, ignores invalid outputs when checking correctness. Defaults to False.

    Returns
    -------
    incorrect : bool
        A boolean indicating if the buggy solution is incorrect.
    results : list
        A list of results from the correctness checks.
    mismatched : list[dict[Literal["input", "output", "buggy_output"], str]] | None
        A list of mismatched input-output pairs for the problematic_inputs if produced,
        otherwise None.
    """

    logger = getLogger(__name__)

    # Test the buggy solution against the correctness checks
    results, buggy_outputs = check_correctness(
        datum, buggy_solution, timeout=5, debug=False, return_outputs=True
    )

    if results is None:
        return False, results, None

    fraction_correct = results.count(True) / len(results)
    fraction_errors = buggy_outputs.count(None) / len(buggy_outputs)

    logger.debug(f"{fraction_errors:.0%} errors")
    logger.debug(f"{fraction_correct:.0%} checks passed")

    if (
        fraction_correct < 1
        if ignore_invalid_outputs
        else fraction_correct < (1 - fraction_errors)
    ):
        return True, results, None

    # Generate a new outputs based on the problematic inputs and test the buggy solution
    if len(problematic_inputs) == 0:
        return False, results, None

    _, outputs = check_correctness(
        datum, solution, evaluate=problematic_inputs, timeout=5, debug=False
    )

    if outputs is not None:
        _, buggy_outputs = check_correctness(
            datum, buggy_solution, evaluate=problematic_inputs, timeout=5, debug=False
        )
    else:
        buggy_outputs = outputs

    # Check for matching solutions
    if len(outputs) != len(buggy_outputs):
        raise ValueError(
            f"Outputs and buggy outputs lists must have the same length. Got "
            f"{len(outputs)} and {len(buggy_outputs)}."
        )

    error_count = 0
    num_outputs = 0
    mismatched_indices = []

    for output, buggy_output in zip(outputs, buggy_outputs):
        buggy = False
        # Ignore invalid inputs
        if output is not None:
            num_outputs += 1
            # Check for a a mismatch
            if output != buggy_output:
                if ignore_invalid_outputs:
                    buggy = True
                else:
                    buggy = buggy_output is not None
        if buggy:
            error_count += 1
            mismatched_indices.append(i)

    if num_outputs == 0:
        fraction_matching = 1
    else:
        fraction_matching = (num_outputs - error_count) / num_outputs

    logger.debug(f"{(num_outputs / len(outputs)):.2%} valid inputs")
    logger.debug(f"{fraction_matching:.2%} matching outputs")

    if fraction_matching == 1.0:
        return False, results, None

    else:
        mismatched = []
        for i in mismatched_indices:
            mismatched.append(
                {
                    "input": problematic_inputs[i],
                    "output": outputs[i],
                    "buggy_output": buggy_outputs[i],
                }
            )

    return True, results, mismatched


def _create_empty_cv_dataset() -> datasets.Dataset:
    """Create an empty code validation dataset with the required columns.

    Returns
    -------
    dataset : datasets.Dataset
        An empty dataset with the required columns.
    """

    return datasets.Dataset.from_dict(
        {
            key: []
            for key in [
                "apps_split",
                "apps_problem_id",
                "difficulty",
                "question",
                "solutions",
                "buggy_solutions",
            ]
        }
    )


def _load_cv_dataset(
    config: CodeValidationDatasetConfig, splits: list[str]
) -> datasets.Dataset:
    """Load an existing code validation dataset or create an empty one.

    First try to load the dataset from the Hugging Face Hub, then try to load it from
    the local directory. If the dataset is not found, create an empty dataset.

    Parameters
    ----------
    config : CodeValidationDatasetConfig
        The configuration object for the code validation dataset.
    splits : list[str]
        The list of splits to load.

    Returns
    -------
    buggy_data : datasets.Dataset
        The code validation dataset.

    Raises
    ------
    ValueError
        If neither the Hugging Face Hub repository nor the local directory are
        specified in the configuration.
    """

    # Load existing buggy data (the only split is train, so we specify that)
    if config.pull_repo is not None:
        buggy_data = datasets.load_dataset(config.pull_repo, split="train")
        Path(config.local_dir).mkdir(parents=True, exist_ok=True)
    elif config.local_dir is None:
        raise ValueError(
            "Either pull_repo or local_dir must be specified in the configuration."
        )
    elif all(
        [
            os.path.exists(os.path.join(config.local_dir, f"{split}.jsonl"))
            for split in splits
        ]
    ):
        buggy_data = datasets.load_dataset(config.local_dir)
    else:
        Path(config.local_dir).mkdir(parents=True, exist_ok=True)
        buggy_data = _create_empty_cv_dataset()

    return buggy_data
