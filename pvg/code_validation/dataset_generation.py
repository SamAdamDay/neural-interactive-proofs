"""Module for generating datasets used in code validation systems.

A code validation dataset is generated by taking the APPS dataset and modifying
solutions to create buggy solutions, using language models.

A `CodeValidationDatasetConfig` class is provided to configure the generation of buggy
solutions for a dataset of problems. The `generate_and_save_cv_dataset` function is used
to generate buggy solutions for a given dataset of problems and save the combined
dataset to disk.
"""

from typing import Optional, Literal, Any
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime, timedelta
import json
import os
import sys
from math import floor
import re
import textdistance
import requests
import multiprocessing
from multiprocessing.managers import SyncManager, ListProxy
import importlib.resources
from logging import getLogger
from string import Template

from tqdm import tqdm

import datasets

from openai import OpenAI
from openai.types.chat.chat_completion import Choice as OpenAIChoice

from pvg.constants import CV_DATA_DIR
from pvg.utils.apps_metric import check_correctness
from pvg.utils.env import load_env_once

ORDINALS = [
    "zeroth",
    "first",
    "second",
    "third",
    "fourth",
    "fifth",
    "sixth",
    "seventh",
    "eighth",
    "ninth",
    "tenth",
]


@dataclass
class CodeValidationDatasetConfig:
    """A configuration class for generating datasets used in code validation systems.

    Attributes
    ----------
    model : str
        The model to be used, default is "openai/gpt-4o-mini".
    difficulties : list of str
        List of difficulty levels, default is ["interview", "competition",
        "introductory"].
    split : {'train', 'test'}, optional
        The data split, default is None.
    fraction_to_modify : float
        Fraction of data to modify, default is 0.5.
    max_modifications : int
        Maximum number of modifications, default is 1.
    num_data : int, optional
        Number of data points per split per difficulty level, default is 10000.
    num_problematic_inputs : int
        Number of problematic inputs to request, default is 0.
    system_prompt : str, optional
        System prompt for generating incorrect solutions, default is None.
    max_attempts : int
        Maximum number of attempts to generate a valid buggy solution, default is 10.
    local_dir : str
        Local directory for data storage.
    pull_repo : str, optional
        Repository to pull data from, default is 'lrhammond/buggy-apps'.
    push_repo : str, optional
        Repository to push data to, default is 'lrhammond/buggy-apps'.
    save_after : int, optional
        Number of operations after which to save data, default is 10.

    Methods
    -------
    __post_init__():
        Initializes the system prompt based on the number of problematic inputs.
    """

    model: str = "openai/gpt-4o-mini"
    difficulties: list[str] = field(
        default_factory=lambda: ["interview", "competition", "introductory"]
    )
    split: Optional[Literal["train", "test"]] = None
    fraction_to_modify: float = 0.5
    max_modifications: int = 1
    num_data: Optional[int] = 10000  # per split per difficulty level
    num_problematic_inputs: int = 0
    system_prompt: Optional[str] = None
    max_attempts: int = 10
    local_dir: str = CV_DATA_DIR
    pull_repo: Optional[str] = "lrhammond/buggy-apps"
    push_repo: Optional[str] = "lrhammond/buggy-apps"
    save_after: Optional[int] = 10

    def __post_init__(self):
        """Set the system prompt based on the number of problematic inputs.

        This method sets the `system_prompt` attribute based on the value of
        `num_problematic_inputs`. If `system_prompt` is already provided, it does
        nothing. Otherwise, it generates a prompt for generating incorrect solutions and
        problematic inputs for a code validation system.

        - If `num_problematic_inputs` is less than 0 or greater than 10, it raises a
          ValueError.
        - If `num_problematic_inputs` is 0, it generates a prompt for modifying a
          solution without providing problematic inputs.
        - If `num_problematic_inputs` is between 1 and 10, it generates a prompt for
          modifying a solution and provides placeholders for the specified number of
          problematic inputs.

        Raises
        ------
        ValueError
            If `num_problematic_inputs` is not between 0 and 10.
        """

        if self.system_prompt is None:

            prompt_template_traversable = importlib.resources.files(
                "pvg.code_validation.prompt_templates.dataset_generation"
            )

            if not 0 <= self.num_problematic_inputs <= 10:
                raise ValueError("num_problematic_inputs must be between 0 and 10")

            elif self.num_problematic_inputs == 0:
                template = Template(
                    prompt_template_traversable.joinpath(
                        "no_problematic_inputs"
                    ).read_text()
                )
                self.system_prompt = template.substitute()

            else:
                problematic_inputs = "\n\n".join(
                    [
                        f"PROBLEMATIC INPUT {i+1}:\n<your {ORDINALS[i]} problematic "
                        f"input here>"
                        for i in range(self.num_problematic_inputs)
                    ]
                )

                template = Template(
                    prompt_template_traversable.joinpath(
                        "with_problematic_inputs"
                    ).read_text()
                )
                self.system_prompt = template.substitute(
                    problematic_inputs=problematic_inputs
                )


def generate_and_save_cv_dataset(
    config: CodeValidationDatasetConfig | dict,
    manager: Optional[SyncManager] = None,
):
    """Generate a code validation dataset and save it to disk.

    This function generates buggy solutions for a given dataset of problems and saves
    the combined dataset to disk. It uses language models to generate buggy solutions
    for a fraction of the solutions provided in the dataset.

    Parameters
    ----------
    config : CodeValidationDatasetConfig | dict
        A configuration object or dictionary for generating the dataset.
    manager : SyncManager, optional
        A multiprocessing manager to handle shared memory, If None, a new manager is
        created.

    Raises
    ------
    ValueError
        If the number of buggy solutions generated is 0.
    """

    logger = getLogger(__name__)

    load_env_once()
    hugging_face_token = os.getenv("HF_TOKEN")

    if isinstance(config, dict):
        config = CodeValidationDatasetConfig(**config)

    # Create process manager in case we get stuck on any of the problems
    if manager is None:
        manager = multiprocessing.Manager()

    process_results = manager.list()

    splits = ["train", "test"] if config.split is None else [config.split]

    # Load existing data
    data = datasets.load_dataset("codeparrot/apps", trust_remote_code=True)
    buggy_data = _load_cv_dataset(config, splits)

    start_time = datetime.now()

    for split in splits:

        # We keep track of which problems we have had trouble generating buggy data for
        # so we can skip them in the future
        problematic_problems_filename = os.path.join(
            config.local_dir, f"problematic_{split}_problems.csv"
        )
        with open(problematic_problems_filename, mode="r") as file:
            line = file.readline().strip()
            problematic_problems = [
                int(value) for value in line.split(",") if value != ""
            ]

        for difficulty in config.difficulties:

            data_slice = (
                data[split]
                .filter(lambda x: x["difficulty"] == difficulty)
                .sort("problem_id")
            )
            buggy_data_slice = buggy_data.filter(
                lambda x: x["difficulty"] == difficulty and x["apps_split"] == split
            ).sort("apps_problem_id")

            num_buggy_data_to_generate = max(
                min(config.num_data, len(data_slice)) - len(buggy_data_slice), 0
            )
            num_data_added = 0

            logger.info(
                f"Generating {num_buggy_data_to_generate} buggy data for the "
                f"{difficulty!r} problems in the {split!r} split"
            )

            new_data = _create_empty_cv_dataset()

            for datum in tqdm(data_slice, colour="green"):

                datum: dict[str, Any]

                if (
                    datum["solutions"] == ""
                    or int(datum["problem_id"]) in problematic_problems
                ):
                    continue

                # The "solutions" field is a JSON string containing a list of solutions
                solutions: list[str] = json.loads(datum["solutions"])

                # Check if we've already generated the buggy data or if there is only
                # one solution given
                if (
                    len(buggy_data_slice) >= num_buggy_data_to_generate
                    or num_data_added >= num_buggy_data_to_generate
                ):
                    break
                elif (
                    datum["problem_id"] in buggy_data_slice["apps_problem_id"]
                    or len(solutions) <= 1
                ):
                    continue

                buggy_datum = {
                    "apps_split": split,
                    "apps_problem_id": datum["problem_id"],
                    "difficulty": difficulty,
                    "question": datum["question"],
                    "solutions": [],
                    "buggy_solutions": [],
                }

                # Generate new buggy solutions
                timeout = min(20 + (5 * len(solutions)), 120)
                process = multiprocessing.Process(
                    target=_generate_buggy_solutions,
                    args=(
                        process_results,
                        datum,
                        config.model,
                        config.system_prompt,
                        config.max_modifications,
                        config.max_attempts,
                    ),
                )
                process.start()
                process.join(timeout)
                if process.is_alive():
                    process.terminate()
                    process.join()
                    logger.warning(
                        f"Generating buggy solutions for problem {datum['problem_id']} "
                        f"({split!r}) timed out after {timeout} seconds"
                    )
                    buggy_solutions = [None]
                else:
                    buggy_solutions = process_results[-1]

                # buggy_solutions = generate_buggy_solutions(
                #     datum,
                #     config.model,
                #     config.openrouter_api_key,
                #     config.system_prompt,
                #     config.max_modifications,
                #     max_attempts=config.max_attempts,
                #     debug=True,
                # )

                # If we didn't manage to generate any buggy solutions, we make a note of
                # this problem so we can skip it in the future
                num_buggy_solutions_generated = len(
                    buggy_solutions
                ) - buggy_solutions.count(None)
                if num_buggy_solutions_generated == 0:
                    with open(problematic_problems_filename, "a") as f:
                        f.write(f"{datum['problem_id']},")
                    continue

                solutions_added = 0
                num_checks = len(json.loads(datum["input_output"])["inputs"])
                for solution_id, (solution, buggy_solution) in enumerate(
                    zip(solutions, buggy_solutions)
                ):
                    if buggy_solution is None:
                        if solutions_added < num_buggy_solutions_generated:
                            solution = {
                                "apps_solution_number": solution_id,
                                "solution": solution,
                                "levenshtein_distance": {"normalised": 0.0, "raw": 0},
                                "checks": {"pass": num_checks, "fail": 0, "error": 0},
                                "generation_attempts": 0,
                            }
                            buggy_datum["solutions"].append(solution)
                            solutions_added += 1
                    else:
                        buggy_datum["buggy_solutions"].append(buggy_solution)

                new_data = new_data.add_item(buggy_datum)
                num_data_added += 1

                # Occasionally save if required
                if config.save_after is None:
                    continue
                elif num_data_added % config.save_after == 0 and num_data_added > 0:
                    # buggy_data.to_json(os.path.join(config.local_dir, s, f"{d}.jsonl"))
                    buggy_data = _load_cv_dataset(config, splits)
                    combined_buggy_data = datasets.concatenate_datasets(
                        [buggy_data, new_data]
                    )
                    new_data = _create_empty_cv_dataset()
                    combined_buggy_data.save_to_disk(
                        os.path.join(config.local_dir, "code_validation.data")
                    )
                    if config.push_repo is not None:
                        combined_buggy_data.push_to_hub(
                            config.push_repo, token=config.token
                        )

            # Calculate the elapsed time, rounding microseconds down
            elapsed_time = datetime.now() - start_time
            elapsed_time = timedelta(
                days=elapsed_time.days, seconds=elapsed_time.seconds
            )
            commit_message = (
                f"Added {num_data_added} buggy data overall in {elapsed_time} for "
                f"the {difficulty} problems in the {split!r} split"
            )
            logger.info(commit_message)

            # Save data
            if num_data_added > 0:
                # buggy_data.to_json(os.path.join(config.local_dir, s, f"{d}.jsonl"))
                buggy_data = _load_cv_dataset(config, splits)
                combined_buggy_data = datasets.concatenate_datasets(
                    [buggy_data, new_data]
                )
                new_data = _create_empty_cv_dataset()
                combined_buggy_data.save_to_disk(
                    os.path.join(config.local_dir, "code_validation.data")
                )
                if config.push_repo is not None:
                    combined_buggy_data.push_to_hub(
                        config.push_repo,
                        commit_message=commit_message,
                        token=hugging_face_token,
                    )


def _generate_buggy_solutions(
    results: ListProxy[list[str | None]],
    datum: list[str, Any],
    model: str,
    system_prompt: str,
    max_modifications: int,
    max_attempts: Optional[int] = None,
) -> ListProxy[list[str | None]]:
    """Generate buggy solutions for a given datum and append them to the result list.

    If an exception is raised during the process, a list containing None is appended to
    the result list.

    Parameters
    ----------
    results : ListProxy[list[str | None]]
        A list proxy to store the buggy solutions.
    datum : list[str, Any]
        The corresponding datum from the original APPS dataset.
    model : str
        The model to use for generating buggy solutions.
    system_prompt : str
        The system prompt to provide context to the model.
    max_modifications : int
        The maximum number of solutions to modify.
    max_attempts : int, optional
        The maximum number of attempts to generate a valid buggy solution, default is
        None.

    Returns
    -------
    results : ListProxy[list[str | None]]
        The list proxy with the buggy solutions appended.
    """

    try:
        buggy_solutions = _try_generate_buggy_solutions(
            datum=datum,
            model=model,
            system_prompt=system_prompt,
            max_modifications=max_modifications,
            max_attempts=max_attempts,
        )
        return results.append(buggy_solutions)
    except Exception:
        return results.append([None])


def _try_generate_buggy_solutions(
    datum: dict,
    model: str,
    system_prompt: str,
    max_modifications: int,
    fraction_to_modify: float = 0.5,
    max_attempts: int = 10,
    multiple_completions: bool = True,
    existing_buggy_solutions: Optional[list[dict]] = None,
) -> list[str | None]:
    """Generate buggy solutions by modifying a fraction of the provided solutions.

    Parameters
    ----------
    datum : dict
        The corresponding datum from the original APPS dataset.
    model : str
        The model to use for generating buggy solutions.
    system_prompt : str
        The system prompt to provide context to the model.
    fraction_to_modify : float
        The fraction of solutions to modify.
    max_modifications : int
        The maximum number of solutions to modify.
    max_attempts : int, optional
        The maximum number of attempts to generate a valid buggy solution. Defaults to
        10.
    multiple_completions : bool, optional
        If True, generate multiple completions for each prompt. Defaults to True.
    existing_buggy_solutions : Optional[list[dict]], optional
        A list of existing buggy solutions to update or extend. Defaults to None.

    Returns
    -------
    list[str | None]
        A list of buggy solutions or None if a valid buggy solution could not be
        generated.
    """

    logger = getLogger(__name__)

    load_env_once()
    openrouter_api_key = os.getenv("OPENROUTER_API_KEY")

    logger.debug(f"Problem: {datum['problem_id']}")

    if existing_buggy_solutions is None:
        buggy_solutions = [None] * len(json.loads(datum["solutions"]))

    solutions = json.loads(datum["solutions"])
    if len(solutions) != len(buggy_solutions):
        raise ValueError(
            f"Both lists of solutions must have the same length. Got {len(solutions)} "
            f"solutions and {len(buggy_solutions)} buggy solutions."
        )

    user_prompt_template = Template(
        "QUESTION:\n\n\n$question\n\nSOLUTION:\n\n\n$solution"
    )

    num_to_modify = min(floor(fraction_to_modify * len(solutions)), max_modifications)
    num_modified = 0

    for solution_id, solution in enumerate(solutions):

        if buggy_solutions[solution_id] is not None:
            continue

        # Check that original solution is valid
        if not all(check_correctness(datum, solution)):
            continue

        user_prompt = user_prompt_template.substitute(
            question=datum["question"], solution=solution
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        valid_buggy_solution = False
        attempts = 0

        logger.debug(f"Solution: {solution_id}/{len(solutions)}")

        while not valid_buggy_solution and attempts < max_attempts:

            logger.debug(f"Attempt: [{attempts+1}/{max_attempts}]")

            if multiple_completions:
                model_outputs = _get_openrouter_response(
                    model, messages, openrouter_api_key, num_responses=max_attempts
                )
                model_outputs = [output["message"] for output in model_outputs]
            else:
                model_outputs = [
                    _get_openrouter_response(model, messages, openrouter_api_key)[0][
                        "message"
                    ]
                ]

            logger.debug("Model output(s) received")

            for model_output in model_outputs:

                attempts += 1

                buggy_solution, problematic_inputs, flag_unsafe = (
                    _extract_code_and_input(model_output)
                )

                logger.debug("Answer extracted")

                if flag_unsafe:
                    logger.debug("Unsafe solution generated")
                    continue

                valid_buggy_solution, input_output_checks, _ = _test_buggy_solution(
                    buggy_solution, solution, problematic_inputs, datum
                )

                if valid_buggy_solution:
                    break

        # if not (True in input_output_checks or False in input_output_checks):
        #     valid_buggy_solution = False

        if valid_buggy_solution:
            logger.debug(f"Valid buggy solution generated in {attempts} attempts")

            # Calculate the Levenshtein distance
            levenshtein_distance = textdistance.levenshtein(solution, buggy_solution)
            normalised_levenstein_distance = levenshtein_distance / max(
                len(solution), len(buggy_solution)
            )
            levenshtein_distance = {
                "normalised": normalised_levenstein_distance,
                "raw": levenshtein_distance,
            }

            passed = input_output_checks.count(True)
            failed = input_output_checks.count(False)

            buggy_solutions[solution_id] = {
                "apps_solution_number": solution_id,
                "solution": buggy_solution,
                "levenshtein_distance": levenshtein_distance,
                "checks": {
                    "pass": passed,
                    "fail": failed,
                    "error": len(input_output_checks) - passed - failed,
                },
                "generation_attempts": attempts,
            }
            num_modified += 1

            if num_modified >= num_to_modify:
                break

        else:
            logger.warning(
                f"Failed to generate buggy solution for solution {solution_id}"
            )
            buggy_solutions[solution_id] = None

    return buggy_solutions


def _get_openrouter_response(
    model: str,
    messages: list[dict[Literal["role", "content"], str]],
    temperature: float = 1.0,
    get_log_probs: bool = False,
    get_top_logprobs: bool = None,
    num_responses: int = 1,
    force_multiple_generations: bool = False,
) -> list[dict[Literal["message", "log_probs", "top_logprobs"], Any]]:
    """Send a POST request to the OpenRouter API to get responses from a chat model.

    Note that this function calls the OpenAI API instead when it can.

    Parameters
    ----------
    model : str
        The name of the chat model to use.
    messages : list
        A list of dictionaries representing the chat messages. Each dictionary should
        have a "role" key with the value "user" or "assistant", and a "content" key with
        the content of the message.
    temperature : float, default=1.0
        The sampling temperature to use when generating completions.
    get_log_probs : bool, default=False
        Whether to return the log probabilities of the tokens in the completion.
    get_top_logprobs : bool, default=None
        Whether to return the top log probabilities of the tokens in the completion.
    num_responses : int, default=1
        The number of completions to generate.
    force_multiple_generations : bool, default=False
        If True, we force multiple generations by making multiple requests to the
        OpenRouter API.

    Returns
    -------
    responses : list[dict[Literal["message", "log_probs", "top_logprobs"], Any]]
        The response object returned by the API.

    Raises
    ------
    requests.exceptions.RequestException
        If there was an error sending the request.
    """

    load_env_once()
    api_key = os.getenv("OPENROUTER_API_KEY")

    responses = []

    # Crazily, the openrouter API doesn't support multiple completions in a single
    # request, so we have to make multiple requests. Also, we don't seem to have access
    # to the the o1 models via the OpenAI API atb the moment?
    if "openai" not in model or "o1" in model or force_multiple_generations:
        completions = []
        for _ in range(num_responses):
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={"Authorization": f"Bearer {api_key}"},
                data=json.dumps(
                    {
                        "model": model,
                        "messages": messages,
                        "temperature": temperature,
                        "logprobs": get_log_probs,
                        "top_logprobs": get_top_logprobs,
                    }
                ),
            )
            completions.append(response.json()["choices"][0])

    else:
        completions = _get_openai_response(
            model.split("/")[-1],
            messages,
            temperature=temperature,
            log_probs=get_log_probs,
            top_logprobs=get_top_logprobs,
            num_responses=num_responses,
        )
        completions = [choice.model_dump() for choice in completions]

    for completion in completions:

        response_dict = {}
        response_dict["message"] = completion["message"]["content"]

        has_log_probs = "logprobs" in completion and completion["logprobs"] is not None

        if get_log_probs:
            if not has_log_probs:
                raise RuntimeError(
                    "Log probabilities were requested but not returned by the model."
                )
            response_dict["log_probs"] = [
                {key: content[key] for key in ["token", "logprob"]}
                for content in completion["logprobs"]["content"]
            ]
        else:
            response_dict["log_probs"] = None

        if get_top_logprobs:
            if not has_log_probs:
                raise RuntimeError(
                    "Top log probabilities were requested but not returned by the "
                    "model."
                )
            response_dict["top_logprobs"] = [
                [
                    {key: position[key] for key in ["token", "logprob"]}
                    for position in content["top_logprobs"]
                ]
                for content in completion["logprobs"]["content"]
            ]
        else:
            response_dict["top_logprobs"] = None

        responses.append(response_dict)

    return responses


def _get_openai_response(
    model: str,
    messages: list[dict[Literal["role", "content"], str]],
    temperature: float = 1.0,
    log_probs: bool = False,
    top_logprobs: bool = None,
    num_responses: int = 1,
) -> list[OpenAIChoice]:
    """Get completions from the OpenAI API for a chat model.

    Parameters
    ----------
    model : str
        The name of the chat model to use.
    messages : list[dict[Literal["role", "content"], str]]
        A list of dictionaries representing the chat messages. Each dictionary should
        have a "role" key with the value "user" or "assistant", and a "content" key with
        the content of the message.
    temperature : float, default=1.0
        The sampling temperature to use when generating completions.
    log_probs : bool, default=False
        Whether to return the log probabilities of the tokens in the completion.
    top_logprobs : bool, default=None
        Whether to return the top log probabilities of the tokens in the completion.
    num_responses : int, default=1
        The number of completions to generate.

    Returns
    -------
    completions : list[OpenAIChoice]
        A list of completions returned by the API.
    """

    load_env_once()
    api_key = os.getenv("OPENAI_API_KEY")

    client = OpenAI(api_key=api_key)

    return client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        logprobs=log_probs,
        top_logprobs=top_logprobs,
        n=num_responses,
    ).choices


def _extract_code_and_input(model_output: str) -> tuple[str | None, list[str], bool]:
    """Extract the modified solution and problematic inputs from the model output.

    This function uses regular expressions to parse the model output and extract:

    1. Problematic inputs.
    2. The modified solution.
    3. A safety check to ensure the modified solution does not contain dangerous
       functions.

    Parameters
    ----------
    model_output : str
        The output from the model containing problematic inputs and the modified
        solution.

    Returns
    -------
    modified_solution : str | None
        The modified solution extracted from the model output, or None if not found.
    problematic_inputs : list[str]
        A list of problematic inputs extracted from the model output.
    flag_unsafe : bool
        A flag indicating if the modified solution contains dangerous functions, and is
        therefore unsafe.
    """

    # Extract the problematic input using regex
    pattern = re.compile(
        r"PROBLEMATIC INPUT \d+:\n(.*?)(?=\nPROBLEMATIC INPUT \d+:|\Z)", re.DOTALL
    )
    problematic_inputs = [
        input.replace("```", "").strip() for input in pattern.findall(model_output)
    ]

    # Extract the modified solution using regex
    if problematic_inputs != []:
        pattern = re.compile(
            r"MODIFIED SOLUTION:\s*(.*?)\s*PROBLEMATIC INPUT 1:", re.DOTALL
        )
    else:
        pattern = re.compile(r"MODIFIED SOLUTION:\s*(.*)", re.DOTALL)
    match = pattern.search(model_output)
    if match:
        modified_solution = (
            match.group(1).replace("python", "").replace("```", "").strip()
        )
    else:
        modified_solution = None

    # Check for dangerous functions
    dangerous_functions = [
        "exit",
        "quit",
        "kill",
        "system",
        "putenv",
        "remove",
        "removedirs",
        "rmdir",
        "fchdir",
        "setuid",
        "fork",
        "forkpty",
        "killpg",
        "rename",
        "renames",
        "truncate",
        "replace",
        "unlink",
        "fchmod",
        "fchown",
        "chmod",
        "chown",
        "chroot",
        "lchflags",
        "lchmod",
        "lchown",
        "getcwd",
        "chdir",
        "rmtree",
        "move",
        "chown",
        "Popen",
        "__builtins__",
        "ipdb",
        "joblib",
        "resource",
        "psutil",
        "tkinter",
    ]
    flag_unsafe = any(func in modified_solution for func in dangerous_functions)

    return modified_solution, problematic_inputs, flag_unsafe


def _test_buggy_solution(
    buggy_solution: str,
    solution: str,
    problematic_inputs: list[str],
    datum: dict,
    ignore_invalid_outputs: bool = False,
) -> tuple[
    bool,
    list,
    Optional[list[dict[Literal["input", "output", "buggy_output"], str]]],
]:
    """Test a buggy solution against a correct solution using provided inputs and datum.

    Parameters
    ----------
    buggy_solution : str
        The buggy solution code to be tested.
    solution : str
        The correct solution code for comparison.
    problematic_inputs : list of str
        A list (possibly empty) of inputs that are predicted to cause issues.
    datum : dict
        The corresponding datum from the original APPS dataset.
    ignore_invalid_outputs : bool, optional
        If True, ignores invalid outputs when checking correctness. Defaults to False.

    Returns
    -------
    incorrect : bool
        A boolean indicating if the buggy solution is incorrect.
    results : list
        A list of results from the correctness checks.
    mismatched : list[dict[Literal["input", "output", "buggy_output"], str]] | None
        A list of mismatched input-output pairs for the problematic_inputs if produced,
        otherwise None.
    """

    logger = getLogger(__name__)

    # Test the buggy solution against the correctness checks
    results, buggy_outputs = check_correctness(
        datum, buggy_solution, timeout=5, debug=False, return_outputs=True
    )

    if results is None:
        return False, results, None

    fraction_correct = results.count(True) / len(results)
    fraction_errors = buggy_outputs.count(None) / len(buggy_outputs)

    logger.debug(f"{fraction_errors:.0%} errors")
    logger.debug(f"{fraction_correct:.0%} checks passed")

    if (
        fraction_correct < 1
        if ignore_invalid_outputs
        else fraction_correct < (1 - fraction_errors)
    ):
        return True, results, None

        # in_outs = json.loads(datum["input_output"])
        # mismatched = []
        # for i in range(len(results)):
        #     buggy = False
        #     if results[i] is not True:
        #         if ignore_invalid_outputs:
        #             buggy = True
        #         else:
        #             buggy = buggy_outputs[i] is not None
        #         if buggy:
        #             mismatched.append(
        #                 {
        #                     "input": in_outs['inputs'][i],
        #                     "output": in_outs['outputs'][i],
        #                     "buggy_output": buggy_outputs[i],
        #                 }
        #             )

    # Generate a new outputs based on the problematic inputs and test the buggy solution
    if len(problematic_inputs) == 0:
        return False, results, None

    _, outputs = check_correctness(
        datum, solution, evaluate=problematic_inputs, timeout=5, debug=False
    )

    if outputs is not None:
        _, buggy_outputs = check_correctness(
            datum, buggy_solution, evaluate=problematic_inputs, timeout=5, debug=False
        )
    else:
        buggy_outputs = outputs

    # Check for matching solutions
    if len(outputs) != len(buggy_outputs):
        raise ValueError(
            f"Outputs and buggy outputs lists must have the same length. Got "
            f"{len(outputs)} and {len(buggy_outputs)}."
        )

    error_count = 0
    num_outputs = 0
    mismatched_indices = []

    for output, buggy_output in zip(outputs, buggy_outputs):
        buggy = False
        # Ignore invalid inputs
        if output is not None:
            num_outputs += 1
            # Check for a a mismatch
            if output != buggy_output:
                if ignore_invalid_outputs:
                    buggy = True
                else:
                    buggy = buggy_output is not None
        if buggy:
            error_count += 1
            mismatched_indices.append(i)

    if num_outputs == 0:
        fraction_matching = 1
    else:
        fraction_matching = (num_outputs - error_count) / num_outputs

    logger.debug(f"{(num_outputs / len(outputs)):.2%} valid inputs")
    logger.debug(f"{fraction_matching:.2%} matching outputs")

    if fraction_matching == 1.0:
        return False, results, None

    else:
        mismatched = []
        for i in mismatched_indices:
            mismatched.append(
                {
                    "input": problematic_inputs[i],
                    "output": outputs[i],
                    "buggy_output": buggy_outputs[i],
                }
            )

    return True, results, mismatched


def _create_empty_cv_dataset() -> datasets.Dataset:
    """Create an empty code validation dataset with the required columns.

    Returns
    -------
    dataset : datasets.Dataset
        An empty dataset with the required columns.
    """

    return datasets.Dataset.from_dict(
        {
            key: []
            for key in [
                "apps_split",
                "apps_problem_id",
                "difficulty",
                "question",
                "solutions",
                "buggy_solutions",
            ]
        }
    )


def _load_cv_dataset(
    config: CodeValidationDatasetConfig, splits: list[str]
) -> datasets.Dataset:
    """Load an existing code validation dataset or create an empty one.

    First try to load the dataset from the Hugging Face Hub, then try to load it from
    the local directory. If the dataset is not found, create an empty dataset.

    Parameters
    ----------
    config : CodeValidationDatasetConfig
        The configuration object for the code validation dataset.
    splits : list[str]
        The list of splits to load.

    Returns
    -------
    buggy_data : datasets.Dataset
        The code validation dataset.

    Raises
    ------
    ValueError
        If neither the Hugging Face Hub repository nor the local directory are
        specified in the configuration.
    """

    # Load existing buggy data (the only split is train, so we specify that)
    if config.pull_repo is not None:
        buggy_data = datasets.load_dataset(config.pull_repo, split="train")
        Path(config.local_dir).mkdir(parents=True, exist_ok=True)
    elif config.local_dir is None:
        raise ValueError(
            "Either pull_repo or local_dir must be specified in the configuration."
        )
    elif all(
        [
            os.path.exists(os.path.join(config.local_dir, f"{split}.jsonl"))
            for split in splits
        ]
    ):
        buggy_data = datasets.load_dataset(config.local_dir)
    else:
        Path(config.local_dir).mkdir(parents=True, exist_ok=True)
        buggy_data = _create_empty_cv_dataset()

    return buggy_data


def _change_to_new_cv_dataset(max_solutions: int = 1) -> datasets.Dataset:

    logger = getLogger(__name__)

    data = datasets.load_dataset(
        "codeparrot/apps",
        trust_remote_code=True,
    )

    buggy_data = datasets.load_dataset("lrhammond/buggy-apps")

    new_dataset = _create_empty_cv_dataset()

    for split in ["train", "test"]:

        indices = buggy_data[split]["problem_id"]
        sliced_data = data[split].filter(lambda x: x["problem_id"] in indices)

        logger.info(f"Split: {split!r}")

        for buggy_datum, datum in tqdm(zip(buggy_data[split], sliced_data)):

            if buggy_datum["problem_id"] != datum["problem_id"]:
                raise ValueError("The data is not aligned.")

            solutions: list[str] = json.loads(datum["solutions"])
            buggy_solutions = json.loads(buggy_datum["solutions"])

            # Don't add items for problems with only one solution
            if len(solutions) <= 1:
                continue

            new_datum = {
                "apps_split": split,
                "apps_problem_id": datum["problem_id"],
                "difficulty": datum["difficulty"],
                "question": datum["question"],
                "solutions": [],
                "buggy_solutions": [],
            }

            num_checks = len(json.loads(datum["input_output"])["inputs"])

            # Add at most 10 pairs of solutions, and keep the dataset balanced
            num_pairs_to_add = min(
                max_solutions,
                len(buggy_solutions) - buggy_solutions.count(None),
                buggy_solutions.count(None),
            )
            added = {"solutions": 0, "buggy_solutions": 0}

            for solution_id, (solution, buggy_solution) in enumerate(
                zip(solutions, buggy_solutions)
            ):

                if buggy_solution is not None:

                    if added["buggy_solutions"] >= num_pairs_to_add:
                        continue

                    buggy_solution_dict = json.loads(buggy_solution)
                    passed = buggy_solution_dict["input_output_checks"].count(True)
                    failed = buggy_solution_dict["input_output_checks"].count(False)
                    # if max(passed, failed) == 0:
                    #     raise ValueError("No checks were performed")

                    new_solution = {
                        "apps_solution_number": solution_id,
                        "solution": buggy_solution_dict["solution"],
                        "levenshtein_distance": buggy_solution_dict[
                            "levenshtein_distance"
                        ],
                        "checks": {
                            "pass": passed,
                            "fail": failed,
                            "error": num_checks - passed - failed,
                        },
                        "generation_attempts": buggy_solution_dict[
                            "generation_attempts"
                        ],
                    }

                    new_datum["buggy_solutions"].append(new_solution)
                    added["buggy_solutions"] += 1

                else:

                    if added["solutions"] >= num_pairs_to_add:
                        continue

                    new_solution = {
                        "apps_solution_number": solution_id,
                        "solution": solution,
                        "levenshtein_distance": {"normalised": 0.0, "raw": 0},
                        "checks": {"pass": num_checks, "fail": 0, "error": 0},
                        "generation_attempts": 0,
                    }

                    new_datum["solutions"].append(new_solution)
                    added["solutions"] += 1

            new_dataset = new_dataset.add_item(new_datum)

    new_dataset.save_to_disk(os.path.join(CV_DATA_DIR, "new_cv_dataset.data"))
    new_dataset.to_json(os.path.join(CV_DATA_DIR, "new_cv_dataset.jsonl"))

    return new_dataset


def _suppress_output(func: callable) -> callable:
    """Suppress stdout and stderr during function execution."""

    def wrapper(*args, **kwargs):
        with open(os.devnull, "w") as fnull:
            original_stdout = sys.stdout
            original_stderr = sys.stderr
            sys.stdout = fnull
            sys.stderr = fnull
            result = func(*args, **kwargs)
            sys.stdout = original_stdout
            sys.stderr = original_stderr
        return result

    return wrapper


# TODO (work in progress, original version that is useful for more intelligently updating or extending the buggy dataset)
def _update_cv_dataset(config: CodeValidationDatasetConfig | dict):

    return

    # return

    if isinstance(config, dict):
        config = CodeValidationDatasetConfig(**config)

    split = ["train", "test"] if config.split is None else [config.split]

    # Load original data
    data = datasets.load_dataset(
        "codeparrot/apps",
        trust_remote_code=True,
        split=config.split,
    )

    # Load existing buggy data
    if config.pull_repo is not None:
        buggy_data = datasets.load_dataset(config.pull_repo, split=config.split)
    else:
        if config.local_dir is None:
            raise ValueError(
                "local_dir must be specified if pull_repo is not specified"
            )
        elif os.path.exists(config.local_dir):
            buggy_data = datasets.load_dataset(
                "json", data_dir=config.local_dir, split=config.split
            )
        else:
            all_data = (
                data
                if config.split is None
                else datasets.load_dataset("codeparrot/apps", trust_remote_code=True)
            )
            buggy_data = datasets.DatasetDict(
                {
                    s: datasets.Dataset.from_dict(
                        {k: all_data[s][k] for k in ["problem_id", "difficulty"]}
                        | {"solutions": [""] * len(all_data[s])}
                    )
                    for s in ["train", "test"]
                }
            )

    # To help with indexing
    if config.split is not None:
        buggy_data = {s: buggy_data for s in split}
        data = {s: data for s in split}

    for s in split:
        if len(data[s]) != len(buggy_data[s]):
            raise ValueError("The lengths of the original and buggy data do not match")

    num_data_added = 0
    num_data_updated = 0
    start_time = datetime.now()

    for s in split:
        # print(f"Generating {len(config.data_range)} buggy data for the {s} split")
        counters = {d: 0 for d in config.difficulties}
        for i in range(len(data[s])):

            datum = data[s][i]
            d = datum["difficulty"]

            # Skip difficulties not in the list
            if d not in config.difficulties:
                continue
            # Skip if we have already generated enough buggy data for this difficulty level
            if counters[d] >= config.num_data:
                continue

            # Sanity check matching problem IDs and difficulties
            buggy_datum = buggy_data[s][i]
            if buggy_datum["problem_id"] != datum["problem_id"]:
                raise ValueError("Problem IDs do not match")
            if buggy_datum["difficulty"] != d:
                raise ValueError("Difficulties do not match")

            # Check if have already generated buggy solutions
            if buggy_datum["solutions"] == "":
                buggy_solutions = [None] * len(json.loads(datum["solutions"]))
                num_data_added += 1
            else:
                buggy_solutions = json.loads(buggy_datum["solutions"])
                if (
                    buggy_solutions.count(None) / len(buggy_solutions)
                    <= 1 - config.fraction_to_modify
                ):
                    continue
                num_data_updated += 1

            # Generate new buggy solutions
            new_buggy_solutions = _try_generate_buggy_solutions(
                datum,
                buggy_solutions,
                config.model,
                config.system_prompt,
                config.fraction_to_modify,
            )
            # buggy_data.map(lambda x: x["solutions"] = json.dumps(new_buggy_solutions))
            buggy_data[s][i]["solutions"] = json.dumps(new_buggy_solutions)

            # Update the counters
            counters[d] += 1

            # Save if required
            if config.save_after is None:
                continue
            elif sum(counters.values()) % config.save_after == 0:
                Path(config.local_dir).mkdir(parents=True, exist_ok=True)
                buggy_data[s].to_json(os.path.join(config.local_dir, f"{s}.jsonl"))
                if config.push_repo is not None:
                    buggy_data[s].push_to_hub(config.push_repo, token=config.token)

    # Calculate the elapsed time, rounding microseconds down
    elapsed_time = datetime.now() - start_time
    elapsed_time = timedelta(days=elapsed_time.days, seconds=elapsed_time.seconds)
    print(f"Done in {elapsed_time}")  # noqa: T201

    # Add new buggy data to the existing buggy data
    # for s in config.split:
    #     with open(os.path.join(config.local_dir, f"{s}.jsonl"), 'a') as file:
    #         for d in new_buggy_data[s]:
    #             # Convert the dictionary to a JSON string and write it to the file
    #             file.write(json.dumps(d) + '\n')

    # Save
    if sum(counters.values()) > 0:
        Path(config.local_dir).mkdir(parents=True, exist_ok=True)
        for s in split:
            buggy_data[s].to_json(os.path.join(config.local_dir, f"{s}.jsonl"))
            if config.push_repo is not None:
                buggy_data.push_to_hub(config.push_repo, split=s)

            new_datum = {  # noqa: F841
                "apps_split": split,
                "apps_problem_id": datum["problem_id"],
                "difficulty": datum["difficulty"],
                "question": datum["question"],
                "solutions": [],
                "buggy_solutions": [],
            }

            num_checks = len(json.loads(datum["input_output"])["inputs"])

            # Add at most 10 pairs of solutions, and keep the dataset balanced
            num_pairs_to_add = min(
                max_solutions,
                len(buggy_solutions) - buggy_solutions.count(None),
                buggy_solutions.count(None),
            )
            added = {"solutions": 0, "buggy_solutions": 0}

            for i in range(len(solutions)):

                if buggy_solutions[i] is not None:

                    if added["buggy_solutions"] >= num_pairs_to_add:
                        continue

                    buggy_solution = json.loads(buggy_solutions[i])
                    passed = buggy_solution["input_output_checks"].count(True)
                    failed = buggy_solution["input_output_checks"].count(False)
                    if max(passed, failed) == 0:
                        raise ValueError("No checks were performed")

                    new_solution = {  # noqa: F841
                        "apps_solution_number": i,
                        "solution": buggy_solution["solution"],
                        "levenshtein_distance": buggy_solution["levenshtein_distance"],
                        "checks": {
                            "pass": passed,
                            "fail": failed,
                            "error": num_checks - passed - failed,
                        },
                        "generation_attempts": buggy_solution["generation_attempts"],
                    }
