"""Code validation agent components.

To integrate easily into the codebase, the code validation agents a split into
three parts: the body, the policy head, and the value head. However, for text
classification, the split is not necessary, so only the policy head actually does
anything. The body and value head return the input data unchanged.
"""

from typing import Optional, Literal, ClassVar, Any, Iterable
from string import Template
from itertools import product
from dataclasses import dataclass
from random import randrange
from tempfile import TemporaryDirectory
from pathlib import Path
import json
from time import sleep
from collections import OrderedDict
from warnings import warn

from torch import from_numpy

import numpy as np
from numpy.typing import NDArray

from einops import rearrange

from jaxtyping import Bool

from openai import (
    OpenAI,
    APITimeoutError,
    APIStatusError,
    RateLimitError,
    APIConnectionError,
)
from openai.types.fine_tuning import FineTuningJob as OpenAIFineTuningJob

from pvg.scenario_base.agents import (
    WholeAgent,
    PureTextWholeAgent,
    PureTextSharedModelGroup,
    RandomWholeAgent,
    CombinedWhole,
    PureTextCombinedWhole,
    Agent,
    PureTextSharedModelGroupState,
)
from pvg.scenario_base.environment import PromptMessage, PureTextEnvironment
from pvg.parameters import (
    HyperParameters,
    CodeValidationAgentParameters,
    RandomAgentParameters,
    ScenarioType,
)
from pvg.experiment_settings import ExperimentSettings
from pvg.factory import register_scenario_class
from pvg.protocols import ProtocolHandler
from pvg.utils.nested_array_dict import NestedArrayDict
from pvg.utils.types import NumpyStringDtype, String
from pvg.utils.env import load_env_once
from pvg.utils.string import random_string
from pvg.utils.api import (
    GenerationError,
    InvalidResponseError,
    ContentFilterError,
    UnknownFinishReasonError,
)
from pvg.constants import WANDB_OPENAI_FINETUNE_PROJECT
from pvg.code_validation.protocols import (
    CodeValidationProtocolHandler,
    CodeValidationAgentSpec,
)


class AgentNotActiveInChannelError(Exception):
    """Error raised when an agent is not active in a channel."""


@dataclass
class OpenAiSharedModelGroupState(PureTextSharedModelGroupState):
    """The state of an OpenAI shared model group.

    Attributes
    ----------
    fine_tune_job_id : Optional[str]
        The ID of the OpenAI API fine-tune job.
    fine_tuned_model_name : Optional[str]
        The name of the most recently fine-tuned model.
    """

    fine_tune_job_id: Optional[str] = None
    fine_tuned_model_name: Optional[str] = None


@dataclass
class _ParsedChatCompletion:
    """A parsed chat completion output.

    Attributes
    ----------
    next_messages : OrderedDict[str, str] | None
        A dictionary mapping channel names to next messages, ordered by channel order.
        If the model has made a decision, this will be None.
    raw_message : str
        The raw message generated by the model, before parsing.
    prompt : list[PromptMessage]
        The prompt used to generate the message.
    decision : int
        The decision made by the agent. This is either 0 (reject), 1 (accept) or 2 (no
        decision).
    retry_count : int
        The number of retries before the generation was successful.
    warning : Literal["max_tokens", "invalid_response"] | None
        The warning message from the generation, if any. One of:

        - "max_tokens": If the generation was stopped because the maximum number of
            tokens was reached.
        - "invalid_response": If the generation could not be parsed.
    """

    next_messages: OrderedDict[str, str] | None
    raw_message: str
    prompt: list[PromptMessage]
    decision: int
    retry_count: int
    warning: Literal["max_tokens", "invalid_response"] | None


@register_scenario_class("code_validation", WholeAgent, {"model_provider": "OpenAI"})
class OpenAiWholeAgent(PureTextWholeAgent):
    """The whole agent for code validation, using OpenAI's API."""

    agent_params: CodeValidationAgentParameters
    protocol_handler: CodeValidationProtocolHandler

    env_level_in_keys = [
        "message_history",
        "message_agent_id",
        "raw_message_history",
        "question",
        "solution",
        "round",
        "seed",
        "prover_stance",
    ]
    agent_level_out_keys = [
        "message",
        "raw_message",
        "prompt",
        "decision",
        "retry_count",
        "token_limit",
    ]

    @property
    def client(self) -> OpenAI:
        """The OpenAI client to use for interacting with the OpenAI API."""
        if self._openai_client is None:
            self._openai_client = OpenAI()
        return self._openai_client

    @property
    def base_model_name(self) -> str:
        """The base OpenAI model name, before any fine-tuning."""
        return self.shared_model_group.base_model_name

    @property
    def model_name(self) -> str:
        """The OpenAI model name, including any fine-tuning."""
        return self.shared_model_group.model_name

    @property
    def agent_spec(self) -> CodeValidationAgentSpec:
        """The `CodeValidationAgentSpec` for the agent."""
        return self.protocol_handler.agent_specs[self.agent_name]

    @property
    def system_prompt_template(self) -> Template:
        """The template for the system prompt."""
        return self.protocol_handler.get_agent_system_prompt_template(self.agent_name)

    def __init__(
        self,
        hyper_params: HyperParameters,
        settings: ExperimentSettings,
        agent_name: str,
        protocol_handler: ProtocolHandler | CodeValidationProtocolHandler,
    ):

        if not isinstance(protocol_handler, CodeValidationProtocolHandler):
            raise NotImplementedError(
                f"The code validation scenario is not implemented for "
                f"{hyper_params.scenario}, because a `CodeValidationProtocolHandler` "
                f"subclass has not been registered."
            )

        super().__init__(
            hyper_params=hyper_params,
            settings=settings,
            agent_name=agent_name,
            protocol_handler=protocol_handler,
        )

        # Make sure the environment variables are loaded, so that we can access the
        # OpenAI API key
        load_env_once()

        self._openai_client: Optional[OpenAI] = None

    def build_fine_tune_dataset(
        self,
        rollouts: NestedArrayDict | list[NestedArrayDict],
        replace_verifier_guess_with_true_label: bool = False,
    ) -> list[dict[Literal["messages"], list[PromptMessage]]]:
        """Build the dataset for fine-tuning the agent given sampled rollouts.

        This method generates a dataset of examples ready to pass to the fine-tune API.

        Parameters
        ----------
        rollouts : NestedArrayDict | list[NestedArrayDict]
            The sampled rollouts. Either a nested dictionary of arrays, or a list
            thereof, with keys:

            - "round" (batch round): The current round number.
            - ("next", "message_history") (batch round round channel): The history of
              messages exchanged between the agents in each channel.
            - ("next", "message_agent_id") (batch round round channel): The id of the
              agent who messaged at a round-channel pair.
            - ("next", "raw_message_history") (batch round round agent): The raw message
              generated by each model in each timestep.
            - "question" (batch round): The problem text.
            - "solution" (batch round): The proposed solution text.
            - "prover_stance" (batch round): When randomizing the prover stance, the
              verdict that the prover is arguing for, where 0 means "reject" and 1 means
              "accept".

        replace_verifier_guess_with_true_label : bool, default=False
            Whether to replace the verifier's guess with the true label. If this is set
            to True, and the agent is a verifier, the verifier's guess will be replaced
            with either 'Decision: accept' or 'Decision: reject' based on the true
            label.

        Returns
        -------
        fine_tune_dataset : list[dict[str, list[PromptMessage]]]
            The dataset for fine-tuning the agent. This is a list of examples, where
            each example is a dictionary with key "messages", whose value is a list of
            dictionaries with keys "role" and "content".
        """

        batch_size = rollouts.batch_size[:-1]

        fine_tune_dataset: list[dict[Literal["messages"], list[PromptMessage, str]]] = (
            []
        )

        def rollout_iterate(
            rollouts: NestedArrayDict | list[NestedArrayDict],
        ) -> Iterable[NestedArrayDict]:
            if isinstance(rollouts, list):
                for rollout in rollouts:
                    yield rollout
            else:
                for batch_id in product(*(range(dim) for dim in batch_size)):
                    yield rollouts[batch_id]

        for rollout in rollout_iterate(rollouts):

            # Get the environment state for the final round
            rollout_final_state = rollout[-1]

            try:
                chat_messages_prompt = self._build_chat_messages_prompt(
                    message_history=rollout_final_state["next", "message_history"],
                    message_agent_id=rollout_final_state["next", "message_agent_id"],
                    raw_message_history=rollout_final_state[
                        "next", "raw_message_history"
                    ],
                    round_id=int(rollout_final_state["round"]),
                    question=rollout_final_state["question"],
                    solution=rollout_final_state["solution"],
                    prover_stance=int(rollout_final_state["prover_stance"]),
                    seed=int(rollout_final_state["seed"]),
                    y=int(rollout_final_state["y"]),
                    ensure_last_message_is_assistant=True,
                    replace_last_message_with_true_label=(
                        self.is_verifier and replace_verifier_guess_with_true_label
                    ),
                )
            except AgentNotActiveInChannelError:
                continue

            fine_tune_dataset.append(dict(messages=chat_messages_prompt))

        return fine_tune_dataset

    def forward(
        self, data: NestedArrayDict, environment: PureTextEnvironment
    ) -> NestedArrayDict:
        """Forward pass through the agent policy head.

        Parameters
        ----------
        data : NestedArrayDict
            The input data. A nested dictionary of arrays with keys:

            - "round" (...): The current round number.
            - "seed" (...): The per-environment seed.
            - "message_history" (... round channel): The history of messages exchanged
                between the agents in each channel.
            - "message_agent_id" (... round channel): The id of the agent who messaged
                at a round-channel pair.
            - "raw_message_history" (... round agent): The raw message generated by each
                model in each timestep.
            - "question" (...): The problem text.
            - "solution" (...): The proposed solution text.
            - "prover_stance" (...): When randomizing the prover stance, the verdict
              that the prover is arguing for, where 0 means "reject" and 1 means
              "accept".

        environment : PureTextEnvironment
            The environment in which the agent is running.

        Returns
        -------
        output_data : NestedArrayDict
            The output of running the agent policy head. This is a nested dictionary of
            arrays with keys:

            - "message" (... channel): The message output by the agent per channel.
            - "raw_message" (...): The raw message generated by the model, before
                parsing.
            - "decision" (...): The decision made by the agent. This is either 0
                (reject), 1 (accept) or 2 (no decision).
            - "retry_count" (... channel): The number of retries before the generation
                was successful.
            - "token_limit" (... channel): A boolean array indicating whether the
                generation was stopped because the maximum number of tokens was reached.
        """

        batch_size = data.batch_size

        output_data = NestedArrayDict(
            dict(
                message=np.full(
                    (*batch_size, self.protocol_handler.num_message_channels),
                    None,
                    dtype=NumpyStringDtype,
                ),
                raw_message=np.full(batch_size, None, dtype=NumpyStringDtype),
                prompt=np.full(
                    (
                        *batch_size,
                        environment.max_prompt_messages,
                        len(PromptMessage.__annotations__),
                    ),
                    None,
                    dtype=NumpyStringDtype,
                ),
                decision=np.full(batch_size, 2, dtype=np.int32),
                retry_count=np.zeros(
                    (*batch_size, self.protocol_handler.num_message_channels),
                    dtype=np.int32,
                ),
                token_limit=np.zeros(
                    (*batch_size, self.protocol_handler.num_message_channels),
                    dtype=np.bool,
                ),
            ),
            batch_size=batch_size,
        )

        active_mask = self.protocol_handler.get_active_agents_mask_from_rounds_and_seed(
            from_numpy(data["round"]), from_numpy(data["seed"])
        )[..., self.agent_index, :]
        active_mask: Bool[NDArray, "... channel"] = active_mask.detach().cpu().numpy()

        for batch_id in product(*(range(dim) for dim in batch_size)):

            # If the agent is not active in any channel, skip
            if not active_mask[batch_id].any():
                continue

            # Generate and store the next message and decision
            parsed_completion = self._generate_next_message_and_decision(
                message_history=data["message_history"][batch_id],
                message_agent_id=data["message_agent_id"][batch_id],
                raw_message_history=data["raw_message_history"][batch_id],
                round_id=data["round"][batch_id],
                question=data["question"][batch_id],
                solution=data["solution"][batch_id],
                seed=int(data["seed"][batch_id]),
                prover_stance=data["prover_stance"][batch_id],
            )
            if parsed_completion.next_messages is None:
                output_data["message"][batch_id] = None
            else:
                output_data["message"][batch_id] = np.array(
                    list(parsed_completion.next_messages.values()),
                    dtype=NumpyStringDtype,
                )
            output_data["raw_message"][batch_id] = parsed_completion.raw_message
            output_data["decision"][batch_id] = parsed_completion.decision
            output_data["prompt"][batch_id] = environment.prompt_list_to_array(
                parsed_completion.prompt
            )
            output_data["retry_count"][batch_id] = parsed_completion.retry_count
            output_data["token_limit"][batch_id] = (
                parsed_completion.warning == "max_tokens"
            )

        return output_data

    def _generate_next_message_and_decision(
        self,
        message_history: String[NDArray, "round channel"],
        message_agent_id: String[NDArray, "round channel"],
        raw_message_history: String[NDArray, "round agent"],
        round_id: int,
        question: str,
        solution: str,
        seed: int,
        prover_stance: int,
    ) -> _ParsedChatCompletion:
        """Generate the next message and decision for the agent, with retries.

        This message takes a single message history and builds and runs the API request
        to generate the next action, which can be a message or a decision.

        If the there is an error in the generation, this method will retry a number of
        times before raising an exception (detailed below).

        Parameters
        ----------
        message_history : String[NDArray, "round channel"]
            The array of messages in the chat history.
        message_agent_id : String[NDArray, "round channel"]
            The id of the agent who messaged at a round-channel pair.
        raw_message_history : String[NDArray, "round agent"]
            The raw message generated by each model in each timestep.
        round_id : int
            The current round number.
        channel_name : str
            The name of the message channel.
        question : str
            The problem text.
        solution : str
            The proposed solution text.
        seed : int
            The per-environment seed.
        prover_stance : int
            The verdict that the prover is arguing for, where 0 means "reject" and 1
            means "accept". (Currently ignored.)

        Returns
        -------
        parsed_chat_completion : _ParsedChatCompletion
            The parsed chat completion output. See the `_ParsedChatCompletion` class
            for details.

        Raises
        ------
        ContentFilterError
            If the agent's response is blocked by a content filter.
        UnknownFinishReasonError
            If the agent finishes generating for an unknown reason.
        InvalidResponseError
            If the agent generates a response in an invalid format.
        InvalidDecisionError
            If the agent generates an invalid decision (i.e. not accept or reject).
        """

        chat_messages_prompt = self._build_chat_messages_prompt(
            message_history=message_history,
            message_agent_id=message_agent_id,
            raw_message_history=raw_message_history,
            round_id=round_id,
            question=question,
            solution=solution,
            prover_stance=prover_stance,
            seed=seed,
        )

        # Try the generation a number of times
        num_generation_errors = 0
        num_timeouts = 0
        num_connection_errors = 0
        while True:
            try:

                completion_text, finish_reason = self._make_generation_api_call(
                    chat_messages_prompt
                )

                warning = None

                # Validate the reason for finishing the generation
                if finish_reason == "content_filter":
                    raise ContentFilterError(num_retries=num_generation_errors)
                elif finish_reason == "length":
                    warning = "max_tokens"
                elif finish_reason != "stop":
                    raise UnknownFinishReasonError(
                        num_retries=num_generation_errors, reason=finish_reason
                    )

                next_messages, decision = self.protocol_handler.parse_chat_completion(
                    completion_text=completion_text,
                    agent_name=self.agent_name,
                    round_id=round_id,
                )
                return _ParsedChatCompletion(
                    next_messages=next_messages,
                    raw_message=completion_text,
                    prompt=chat_messages_prompt,
                    decision=decision,
                    retry_count=num_generation_errors,
                    warning=warning,
                )

            except InvalidResponseError as e:
                num_generation_errors += 1
                if (
                    num_generation_errors
                    == self.agent_params.num_invalid_generation_retries
                ):
                    warn(
                        f"After {num_generation_errors} retries, got invalid response "
                        f"from agent {self.agent_name!r}: {e.response_text!r}. Filling "
                        f"with empty actions."
                    )
                    return _ParsedChatCompletion(
                        next_messages=self.protocol_handler.empty_channel_message,
                        raw_message=e.response_text,
                        prompt=chat_messages_prompt,
                        decision=2,
                        retry_count=num_generation_errors,
                        warning="invalid_response",
                    )

            # Retry if there is a generation error
            except GenerationError as e:
                num_generation_errors += 1
                if (
                    num_generation_errors
                    == self.agent_params.num_invalid_generation_retries
                ):
                    raise e.copy_with_retries(num_generation_errors) from e

            # Retry if there is a timeout, but wait a bit first
            except APITimeoutError as e:
                print("API timeout. Retrying in 10 seconds...")  # noqa: T201
                num_timeouts += 1
                if num_timeouts == self.settings.num_api_generation_timeouts:
                    raise e
                sleep(10)

            except APIConnectionError as e:
                sleep_seconds = 0.01 * 2**num_connection_errors
                print(  # noqa: T201
                    f"API Connection error: {e}. Retrying in {sleep_seconds} seconds..."
                )
                num_connection_errors += 1
                if num_connection_errors == self.settings.num_api_connection_errors:
                    raise e
                sleep(sleep_seconds)

    def _build_chat_messages_prompt(
        self,
        message_history: String[NDArray, "round channel"],
        message_agent_id: String[NDArray, "round channel"],
        raw_message_history: String[NDArray, "round agent"],
        round_id: int,
        question: str,
        solution: str,
        prover_stance: int,
        seed: int,
        y: Optional[int] = None,
        ensure_last_message_is_assistant: bool = False,
        replace_last_message_with_true_label: bool = False,
    ) -> list[PromptMessage]:
        """Construct the chat history ready to feed to the API.

        Parameters
        ----------
        message_history : String[NDArray, "round channel"]
            The array of messages in the chat history.
        message_agent_id : String[NDArray, "round channel"]
            The id of the agent who messaged at a round-channel pair.
        raw_message_history : String[NDArray, "round agent"]
            The raw message generated by each model in each timestep.
        round_id : int
            The current round number.
        channel_name : str
            The name of the message channel.
        question : str
            The problem text.
        solution : str
            The proposed solution text.
        seed : int
            The per-environment seed.
        y : int, optional
            The true label (0 for incorrect, 1 for correct). Only used if
            `replace_last_message_with_true_label` is set to True.
        ensure_last_message_is_assistant : bool, default=False
            Whether to ensure the last message is from the assistant, by removing
            messages from the user.
        replace_last_message_with_true_label : bool, default=False
            Whether to replace the last message with 'Decision: accept' or 'Decision:
            reject' based on the true label. If this is set to True, `y` must be
            provided. Only used if `ensure_last_message_is_assistant` is set to True.

        Returns
        -------
        chat_messages : list[PromptMessage]
            The chat messages ready to feed to the API.

        Raises
        ------
        AgentNotActiveInChannelError
            If `ensure_last_message_is_assistant` is set to True and the agent is not
            active in the channel (i.e. there would be no messages in the chat history).
        """

        if replace_last_message_with_true_label:
            if y is None:
                raise ValueError(
                    "If `replace_last_message_with_true_label` is set to True, `y` "
                    "must be provided."
                )
            if not ensure_last_message_is_assistant:
                raise ValueError(
                    "If `replace_last_message_with_true_label` is set to True, "
                    "`ensure_last_message_is_assistant` must also be set to True."
                )
            if not self.is_verifier:
                raise ValueError(
                    "If `replace_last_message_with_true_label` is set to True, the "
                    "agent must be a verifier."
                )

        # First add the system prompt
        system_prompt = self.protocol_handler.get_agent_system_prompt(
            agent_name=self.agent_name,
            question=question,
            solution=solution,
            max_questions=self.protocol_handler.max_verifier_turns - 1,
            max_response_words=self.agent_params.max_response_words,
            agent_stance=prover_stance,
        )
        chat_messages = [dict(role="system", content=system_prompt)]

        # Then add the chat history, with messages from the current agent
        for round_id in range(self.max_message_rounds):

            no_message = True

            # Add the last-round system message if applicable
            if (
                round_id == self.max_message_rounds - 1
                and self.agent_spec.last_round_system_message is not None
            ):
                chat_messages.append(
                    dict(
                        role="system", content=self.agent_spec.last_round_system_message
                    )
                )
                no_message = False

            # Add the current agent's raw message if we're doing that, and the message
            # is not None
            if (
                self.agent_spec.use_raw_message_for_self_prompt
                and raw_message_history[round_id, self.agent_index] is not None
            ):
                chat_messages.append(
                    dict(
                        role="assistant",
                        content=raw_message_history[round_id, self.agent_index],
                    )
                )
                no_message = False

            for channel_name in self.protocol_handler.get_agent_ordered_channels(
                self.agent_name, seed=seed + round_id
            ):

                # If the agent cannot see the channel, skip
                if not self.protocol_handler.can_agent_see_channel(
                    self.agent_name, channel_name
                ):
                    continue

                channel_id = self.protocol_handler.message_channel_names.index(
                    channel_name
                )

                message = message_history[round_id, channel_id]

                # Message is None if no agent sent a message in this channel in this
                # round
                if message is None:
                    continue

                # Determine which agent sent the message
                if message_agent_id[round_id, channel_id] == -1:
                    raise ValueError(f"Agent ID is -1 in round {round_id}")
                message_agent_name = self.protocol_handler.agent_names[
                    message_agent_id[round_id, channel_id]
                ]

                # If the agent which sent the message is the current agent, and we have
                # already added the raw message, skip
                if (
                    self.agent_spec.use_raw_message_for_self_prompt
                    and message_agent_name == self.agent_name
                ):
                    continue

                # Build the item in the chat history
                chat_item = dict(content=str(message))

                # Determine the role of the message
                if self.agent_name == message_agent_name:
                    chat_item["role"] = "assistant"
                else:
                    chat_item["role"] = "user"

                # Add the name of the agent if it is not anonymous
                message_agent_spec = self.protocol_handler.agent_specs[
                    message_agent_name
                ]
                if not message_agent_spec.anonymous:
                    chat_item["name"] = message_agent_spec.human_name

                chat_messages.append(chat_item)
                no_message = False

            # If we didn't add any message this round, this is the end of the history
            if no_message:
                break

        if ensure_last_message_is_assistant:
            while chat_messages[-1]["role"] != "assistant":
                chat_messages.pop()
                if len(chat_messages) == 0:
                    raise AgentNotActiveInChannelError

            if replace_last_message_with_true_label:
                if y == 0:
                    chat_messages[-1]["content"] = "Decision: reject"
                else:
                    chat_messages[-1]["content"] = "Decision: accept"

        return chat_messages

    def _make_generation_api_call(
        self,
        chat_messages_prompt: list[PromptMessage],
    ) -> tuple[str, Literal["stop", "content_filter", "length"]]:
        """Call the OpenAI API to generate the next message.

        Parameters
        ----------
        chat_messages_prompt : list[PromptMessage]
            The message history to feed to the API. A list of dicts with keys "role"
            and "content".

        Returns
        -------
        completion_text : str
            The text of the completion generated by the API.
        finish_reason : Literal["stop", "content_filter", "length"]
            The reason for finishing the generation.
        """

        if self.agent_params.use_dummy_api:
            return self._generate_dummy_response(chat_messages_prompt), "stop"
        else:
            max_tokens = self.agent_params.max_tokens_per_message
            if max_tokens is None:
                max_tokens = int(self.agent_params.max_response_words * 1.5)
            completion = self.client.chat.completions.create(
                model=self.model_name,
                messages=chat_messages_prompt,
                max_tokens=max_tokens,
                temperature=self.agent_params.temperature,
                top_p=self.agent_params.top_p,
            )
            choice = completion.choices[0]
            return choice.message.content, choice.finish_reason

    def _generate_dummy_response(
        self, chat_messages_prompt: list[PromptMessage]
    ) -> str:
        """Generate a dummy response to a chat prompt.

        Parameters
        ----------
        chat_messages_prompt : list[PromptMessage]
            The chat messages prompt to generate a response to.

        Returns
        -------
        response : str
            The dummy response generated.
        """

        if self.is_verifier:

            output_type = randrange(5)

            if output_type == 0:
                return "Decision: reject"
            elif output_type == 1:
                return "Decision: accept"
            else:
                if self.agent_spec.response_channel_headers is None:
                    return f"Question: {random_string(20)}"
                else:
                    response = [
                        f"{header} {random_string(20)}"
                        for header in self.agent_spec.response_channel_headers.values()
                    ]
                    return "\n".join(response)

        else:

            if self.agent_spec.response_channel_headers is None:
                return f"Answer: {random_string(20)}"
            else:
                response = [
                    f"{header} {random_string(20)}"
                    for header in self.agent_spec.response_channel_headers.values()
                ]
                return "\n".join(response)

    def __getstate__(self) -> dict[str, Any]:
        """Get the state of the object for pickling.

        We don't pickle the OpenAI client, as it is not picklable.

        Returns
        -------
        state : dict[str, any]
            The state of the object.
        """

        state = self.__dict__
        state["_openai_client"] = None

        return state


@register_scenario_class(
    "code_validation", PureTextSharedModelGroup, {"model_provider": "OpenAI"}
)
class OpenAiSharedModelGroup(PureTextSharedModelGroup):
    """A class representing a group of code validation OpenAI agents sharing a model."""

    state_class: ClassVar[type[PureTextSharedModelGroupState]] = (
        OpenAiSharedModelGroupState
    )

    agent_wholes: dict[str, OpenAiWholeAgent]

    @property
    def client(self) -> OpenAI:
        """The OpenAI client to use for interacting with the OpenAI API."""
        if self._openai_client is None:
            self._openai_client = OpenAI()
        return self._openai_client

    def __init__(
        self,
        hyper_params: HyperParameters,
        settings: ExperimentSettings,
        protocol_handler: ProtocolHandler | CodeValidationProtocolHandler,
        agent_wholes: dict[str, OpenAiWholeAgent],
        group_name: str,
    ):

        if not isinstance(protocol_handler, CodeValidationProtocolHandler):
            raise NotImplementedError(
                f"The code validation scenario is not implemented for "
                f"{hyper_params.scenario}, because a `CodeValidationProtocolHandler` "
                f"subclass has not been registered."
            )

        super().__init__(
            hyper_params, settings, protocol_handler, agent_wholes, group_name
        )

        # Make sure the environment variables are loaded, so that we can access the
        # OpenAI API key
        load_env_once()

        self._openai_client: Optional[OpenAI] = None

    def create_supervised_fine_tune_job(
        self,
        rollouts_per_agent: dict[str, NestedArrayDict],
        guess_replaced_rollouts: dict[str, NestedArrayDict] = {},
    ):
        """Create a supervised fine-tune job for the agent.

        This method generates a dataset of examples ready to pass to the fine-tune API.

        Parameters
        ----------
        rollouts_per_agent: dict[str, NestedArrayDict]
            The sampled rollouts for each agent. Each is a nested dictionary of arrays
            with keys:

            - "round" (...): The current round number.
            - "message_history" (... round channel): The history of messages exchanged
                between the agents in each channel.
            - "message_agent_id" (... round channel): The id of the agent who messaged
                at a round-channel pair.
            - "raw_message_history" (... round agent): The raw message generated by each
                model in each timestep.
            - "question" (...): The problem text.
            - "solution" (...): The proposed solution text.
            - "y" (...): The true label (0 for incorrect, 1 for correct).
            - "prover_stance" (...): When randomizing the prover stance, the verdict
              that the prover is arguing for, where 0 means "reject" and 1 means
              "accept".

        guess_replaced_rollouts : dict[str, NestedArrayDict], default={}
            Additional rollouts for the verifier agents where the verifier's guess is to
            be replaced with the true label. In these the verifier's guess will be
            replaced with either 'Decision: accept' or 'Decision: reject' based on the
            true label.
        """

        if self.shared_agent_params.freeze_agent:
            self.fine_tune_job_id = "frozen_job_id"
            return

        fine_tune_datasets: dict[
            str, list[dict[Literal["messages"], list[PromptMessage]]]
        ] = {}
        for agent_name, agent_whole in self.agent_wholes.items():
            fine_tune_datasets[agent_name] = agent_whole.build_fine_tune_dataset(
                rollouts_per_agent[agent_name]
            )
            if agent_name in guess_replaced_rollouts:
                if not agent_whole.is_verifier:
                    raise ValueError(
                        f"Guess replaced rollouts can only be provided for verifier "
                        f"agents. But {agent_name!r} is not a verifier agent."
                    )
                fine_tune_datasets[agent_name].extend(
                    agent_whole.build_fine_tune_dataset(
                        guess_replaced_rollouts[agent_name],
                        replace_verifier_guess_with_true_label=True,
                    )
                )

        fine_tune_dataset = sum(fine_tune_datasets.values(), [])

        self._make_fine_tune_api_call(fine_tune_dataset, method="supervised")

    def create_dpo_fine_tune_job(
        self,
        rollouts_per_agent: dict[str, list[NestedArrayDict]],
    ):
        """Create a DPO fine-tune job for the agent group given sampled rollouts.

        This method generates a dataset of examples ready to pass to the fine-tune API.

        Parameters
        ----------
        rollouts_per_agent : dict[str, list[NestedArrayDict]]
            The data for each agent in the group. Each agent's data is a list of
            individual rollouts, which may vary in length.
        """

        if self.shared_agent_params.freeze_agent:
            self.fine_tune_job_id = "frozen_job_id"
            return

        fine_tune_datasets: dict[
            str, list[dict[Literal["messages"], list[PromptMessage]]]
        ] = {}
        for agent_name, agent_whole in self.agent_wholes.items():
            fine_tune_datasets[agent_name] = agent_whole.build_fine_tune_dataset(
                rollouts_per_agent[agent_name]
            )

        fine_tune_dataset = sum(fine_tune_datasets.values(), [])

        self._make_fine_tune_api_call(fine_tune_dataset, method="dpo")

    def get_fine_tune_job_status(
        self,
    ) -> Literal["pending", "running", "succeeded", "failed", "cancelled"]:
        """Get the status of the fine-tune job."""

        if (
            self.shared_agent_params.use_dummy_api
            or self.shared_agent_params.freeze_agent
        ):
            return "succeeded"

        if self.fine_tune_job_id == "insufficient_data_job_id":
            return "succeeded"

        status = self._get_fine_tune_job().status

        if status in ["validating_files", "queued"]:
            return "pending"
        elif status in ["running", "succeeded", "failed", "cancelled"]:
            return status
        else:
            raise ValueError(f"Unknown OpenAI fine-tune job status {status!r}")

    def get_fine_tune_job_error_repr(self) -> str:
        """Get a string representation of the error for the fine-tune job."""

        if self.shared_agent_params.use_dummy_api:
            raise ValueError("Cannot get error for dummy API")

        if self.shared_agent_params.freeze_agent:
            raise ValueError("Cannot get fine-tune error for frozen agent")

        if self.fine_tune_job_id == "insufficient_data_job_id":
            raise ValueError("Cannot get error for insufficient data job")

        error = self._get_fine_tune_job().error

        output = f"Code: {error.code}. Message: {error.message}."
        if error.param is not None:
            output += f" Parameter: {error.param}."

        if isinstance(error, APIStatusError):
            output += f" Headers: {error.response.headers}."

        return output

    def switch_to_next_model(self):
        """Switch to the next model after fine-tuning."""

        if self.fine_tune_job_id is None:
            raise ValueError("Fine-tune job ID not set")

        if self.shared_agent_params.use_dummy_api:
            self.fine_tuned_model_name = "dummy_model_name"
            return

        # If we didn't fine-tune due to insufficient data, don't switch models
        if self.fine_tune_job_id == "insufficient_data_job_id":
            return

        if self.shared_agent_params.freeze_agent:
            self.fine_tuned_model_name = None
            return

        job = self._get_fine_tune_job()

        if job.status != "succeeded":
            raise ValueError(
                f"Cannot switch to next model: fine-tune job status is {job.status!r}"
            )

        if job.fine_tuned_model is None:
            raise ValueError("Fine-tuned model name not set in fine-tune job")

        self.fine_tuned_model_name = job.fine_tuned_model

    def get_state_dict(self) -> dict:
        """Get the state dictionary of the agent.

        Returns
        -------
        state_dict : dict
            The state dictionary of the agent.
        """

        return dict(
            fine_tune_job_id=self.fine_tune_job_id,
            fine_tuned_model_name=self.fine_tuned_model_name,
        )

    def set_state(self, checkpoint: OpenAiSharedModelGroupState | dict[str, Any]):
        """Set the state of the shared model group from a checkpoint.

        Parameters
        ----------
        checkpoint : AgentCheckpoint
            The checkpoint to restore the state from.
        """

        if isinstance(checkpoint, dict):
            checkpoint = OpenAiSharedModelGroupState(**checkpoint)

        self.fine_tune_job_id = checkpoint.fine_tune_job_id
        self.fine_tuned_model_name = checkpoint.fine_tuned_model_name

    def _make_fine_tune_api_call(
        self,
        fine_tune_dataset: list[dict[Literal["messages"], list[PromptMessage]]],
        method: Literal["supervised", "dpo"],
    ):
        """Make the API call to fine-tune the model.

        Parameters
        ----------
        fine_tune_dataset : list[dict[Literal["messages"], list[PromptMessage]]]
            The dataset of examples to fine-tune the model with.
        """

        # OpenAI requires at least 10 examples for fine-tuning
        if len(fine_tune_dataset) < 10:
            self.fine_tune_job_id = "insufficient_data_job_id"
            return

        if self.shared_agent_params.use_dummy_api:
            self.fine_tune_job_id = "dummy_job_id"
            return

        with TemporaryDirectory() as temp_dir:

            # Write the dataset to a temporary file
            file_path = Path(temp_dir, "fine_tune_dataset.jsonl")
            with open(file_path, "w") as f:
                for example in fine_tune_dataset:
                    f.write(json.dumps(example) + "\n")

            # Upload the file to OpenAI
            uploaded_file = self.client.files.create(
                file=open(file_path, "rb"), purpose="fine-tune"
            )

        file_id = uploaded_file.id

        if self.shared_agent_params.fine_tune_from_scratch:
            model_name = self.base_model_name
        else:
            model_name = self.model_name

        # Create the fine-tune job
        while True:
            try:
                job = self.client.fine_tuning.jobs.create(
                    model=model_name,
                    training_file=file_id,
                    integrations=[
                        {
                            "type": "wandb",
                            "wandb": {"project": WANDB_OPENAI_FINETUNE_PROJECT},
                        }
                    ],
                    method={"type": method},
                )

            # If we are day rate limited, sleep for an hour and try again
            except RateLimitError as e:
                if e.code == "daily_rate_limit_exceeded":
                    sleep(60 * 60)
                    continue
                else:
                    raise e

            break

        self.fine_tune_job_id = job.id

    def _get_fine_tune_job(self) -> OpenAIFineTuningJob:
        """Get the fine-tune job from the OpenAI API."""

        if self.fine_tune_job_id is None:
            raise ValueError("Fine-tune job ID not set")

        return self.client.fine_tuning.jobs.retrieve(self.fine_tune_job_id)

    def __getstate__(self) -> dict[str, Any]:
        """Get the state of the object for pickling.

        We don't pickle the OpenAI client, as it is not picklable.

        Returns
        -------
        state : dict[str, any]
            The state of the object.
        """

        state = self.__dict__
        state["_openai_client"] = None

        return state


@register_scenario_class("code_validation", RandomWholeAgent)
class CodeValidationRandomAgentPolicyHead(PureTextWholeAgent, RandomWholeAgent):
    """Random agent for code validation, yielding random strings."""


@register_scenario_class("code_validation", CombinedWhole)
class CodeValidationCombinedWholeAgent(PureTextCombinedWhole):
    """Module which combines all agents for code validation."""

    def forward(
        self, data: NestedArrayDict, environment: PureTextEnvironment
    ) -> NestedArrayDict:
        """Run the forward pass through all agent parts and combine the outputs.

        Parameters
        ----------
        data : NestedArrayDict
            The input data. A nested dictionary of arrays with keys:

            - "round" (...): The current round number.
            - "seed" (...): The per-environment seed.
            - "message_history" (... round channel): The history of messages exchanged
                between the agents in each channel.
            - "message_agent_id" (... round channel): The id of the agent who messaged
                at a round-channel pair.
            - "question" (...): The problem text.
            - "solution" (...): The proposed solution text.
            - "prover_stance" (...): When randomizing the prover stance, the verdict
                that the prover is arguing for, where 0 means "reject" and 1 means
                "accept".

        environment : PureTextEnvironment
            The environment in which the agents are running.

        Returns
        -------
        data : NestedArrayDict
            The input data updated with the output of running the agent. This has the
            added keys:

            - ("agents", "message") (... agent channel): The message generated by the
                agent.
            - ("agents", "raw_message") (... agent): The raw message generated by the
                models, before parsing.
            - ("agents", "prompt") (... agent message field): The prompt used to
                generate the message.
            - ("agents", "decision") (... agent): The decision made by the agent. This
                is either 0 (reject), 1 (accept) or 2 (no decision).
            - ("agents", "retry_count") (... agent channel): The number of retries
                before the generation was successful for each agent.
            - ("agents", "token_limit") (... agent channel): A boolean array indicating
                whether the generation was stopped because the maximum number of tokens
                was reached for each agent.
        """

        whole_outputs: dict[str, NestedArrayDict] = {}
        for agent_name in self.agent_names:

            # Build the input dictionary for the agent. TODO: Restrict message history
            # to visible channels
            input_nad = NestedArrayDict(
                {key: data[key] for key in self.wholes[agent_name].in_keys},
                batch_size=data.batch_size,
            )

            # Run the agent
            whole_outputs[agent_name] = self.wholes[agent_name](input_nad, environment)

        agents_update = {}

        # Stack the outputs
        agents_update["message"] = rearrange(
            [whole_outputs[agent_name]["message"] for agent_name in self.agent_names],
            "agent ... channel -> ... agent channel",
        )
        agents_update["raw_message"] = rearrange(
            [
                whole_outputs[agent_name]["raw_message"]
                for agent_name in self.agent_names
            ],
            "agent ... -> ... agent",
        )
        agents_update["prompt"] = rearrange(
            [whole_outputs[agent_name]["prompt"] for agent_name in self.agent_names],
            "agent ... message field -> ... agent message field",
        )
        agents_update["decision"] = rearrange(
            [whole_outputs[agent_name]["decision"] for agent_name in self.agent_names],
            "agent ... -> ... agent",
        )
        agents_update["retry_count"] = rearrange(
            [
                whole_outputs[agent_name]["retry_count"]
                for agent_name in self.agent_names
            ],
            "agent ... channel -> ... agent channel",
        )
        agents_update["token_limit"] = rearrange(
            [
                whole_outputs[agent_name]["token_limit"]
                for agent_name in self.agent_names
            ],
            "agent ... channel -> ... agent channel",
        )

        data = data.update(dict(agents=agents_update))

        return data

    def _expand_output_to_all_channels(
        self,
        agent_name: str,
        output: NDArray,
        shape_spec: str,
        fill_value: Any = None,
    ) -> NDArray:
        """Expand an agent's output from its visible message channels to all.

        Agents only output to the channels they can see. This function expands the
        output to all channels, by filling in `fill_value` for the logits in the
        channels the agent cannot see.

        Parameters
        ----------
        agent_name : str
            The name of the agent.
        output : NDArray
            An output of an agent. This is a single key in the output of the agent's
            forward pass.
        shape_spec : str
            The shape of the output. This is a space-separated string of the dimensions
            of the output. One of these must be "channel".
        fill_value : Any, default=-1e9
            The value to fill in for the channels the agent cannot see.

        Returns
        -------
        expanded_output : NDArray
            The output expanded to all channels. This has the same shape as `output`,
            except that the channel dimension is the full set of message channels.
        """
        # TODO: Combine this with _expand_logits_to_all_channels

        agent_index = self.agent_names.index(agent_name)

        dim_names = shape_spec.split(" ")

        if dim_names.count("channel") != 1:
            raise ValueError(
                f"The output shape must contain exactly one 'channel' dimension. Got "
                f"{shape_spec!r}."
            )

        channel_dim = dim_names.index("channel")

        if "..." in dim_names[channel_dim + 1 :]:
            raise ValueError(
                f"An ellipsis (...) is not allowed after the 'channel' dimension. Got "
                f"{shape_spec!r}."
            )

        channel_dim = channel_dim - len(dim_names)

        # If the output is already expanded, return it
        if output.shape[channel_dim] == self.protocol_handler.num_message_channels:
            return output

        # Create a tensor filled with `fill_value` of the correct shape
        full_shape = list(output.shape)
        full_shape[channel_dim] = self.protocol_handler.num_message_channels
        expanded_output = np.full(full_shape, fill_value, dtype=output.dtype)

        # Create an index for the tensor, which selects the visible channels using a
        # mask along the channel dimension
        visible_mask = self.protocol_handler.agent_channel_visibility_mask[agent_index]
        index = (Ellipsis, visible_mask) + (slice(None),) * (-1 - channel_dim)

        # Fill in the visible channels
        expanded_output[index] = output

        return expanded_output


@register_scenario_class("code_validation", Agent)
@dataclass
class CodeValidationAgent(Agent):
    """A class representing a code validation agent.

    This is a dataclass which holds all the agent parts.
    """

    agent_params: ClassVar[CodeValidationAgentParameters | RandomAgentParameters]

    agent_state_class: ClassVar[type[OpenAiSharedModelGroupState]] = (
        OpenAiSharedModelGroupState
    )
